---
title: "Evaluation of Clustering Algorithms"
author: "Derek Chiu"
date: "August 18, 2015"
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 3
---

```{r setup, echo=FALSE, cache=FALSE, include=TRUE, message=FALSE}
library(tcgaHGSC)
library(ConClust)
library(clv)
library(clusterSim)
library(mclust)
library(infotheo)
library(ggplot2)
library(RColorBrewer)
library(pander)
library(tidyr)
library(plyr)
library(dplyr)
library(magrittr)
library(knitr)
panderOptions("table.split.table", 300)
opts_chunk$set(echo = FALSE)
```

```{r load}
results.CCP <- readRDS("TCGA/outputs/results_CCP.rds")
results.NMF <- readRDS("TCGA/outputs/results_NMF.rds")
data(hgsc)
dat <- hgsc %>%
  set_rownames(.$UNIQID) %>%
  select(which(sapply(., class) == "numeric")) %>%
  magrittr::extract(apply(., 1, sd) > 1, ) %>%
  t %>%
  scale

ALG.NAMES <- c("HC (Euclidean)", "HC (Diana)",
               "KM (Euclidean)", "KM (Spearman)", "KM (MI)",
               "PAM (Euclidean)", "PAM (Spearman)", "PAM (MI)",
               "NMF (Divergence)", "NMF (Euclidean)")
```

# Introduction
Cluster analysis is the unsupervised learning method of assigning entities into different groups based on one or more of their attributes. The goal is to place similar objects together and separate dissimilar objects. For example, in genomics studies, we frequently try and cluster patient samples measured on a large number of molecular features. When we get a clustering assignment from an algorithm, we often want to evaluate its performance. Ideally, a good clustering algorithm is able to differentiate entities with no knowledge of the true class labels. In addition, we want the algorithm to arrive at a stable and optimal number of clusters. There are two main categories of clustering evaluation: **external evaluation** and **internal evaluation**.

```{r all_clusters}
results.all <- results.CCP %>%
  sapply(., function(x) x[[4]]$consensusClass) %>%
  cbind(., results.NMF$nmfDiv$consensusClass,
        results.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

meta.cm <- consensusMatrix(results.all)
BuPuFun <- colorRampPalette(brewer.pal(n = 9, "BuPu"))
palBuPu <- BuPuFun(256)
heatmap(meta.cm, labRow = NA, labCol = NA, col = palBuPu)
```

The proportion of cases with at least 0.6 agreement is `r ((sum(meta.cm >= 0.6) - ncol(meta.cm)) / 2) / choose(ncol(meta.cm), 2)`.

```{r conf_mat}
final.compare <- hclust(dist(meta.cm), method = "average") %>%
  cutree(4) %>%
  as.factor %>%
  set_names(hclust(dist(meta.cm), method = "average")$labels) %>%
  set_names(substring(names(.), first = 18)) %>%
  table(., names(.)) %>%
  magrittr::extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table

names(dimnames(final.compare)) <- c("Predicted", "Reference")
cm <- caret::confusionMatrix(final.compare)
```

The confusion matrix is shown below, as well as different metrics for each class.

```{r conf_mat_output, results='asis'}
pander::pandoc.table(cm$table)
pander::pandoc.table(cm$byClass)
```

# External Evaluation
External evaluation usually refers to the case when we compare our clustering assignments to true class labels, or have some gold standard to compare to. In applications, this might be the published clustering result. The downside of using external evaluation is that the reference classes may not be correctly clustered themselves, and we are treating these as the norm. None the less, we can explore a few metrics.

## Kappa Statistic

```{r kappa}
kappa <- psych::wkappa(final.compare)
```

The unadjusted kappa statistic is `r kappa$kappa` and the weighted kappa statistic is `r kappa$weighted.kappa` for the final meta consensus cluster.
 
## Adjusted Rand Index

The larger the better.

```{r ARI, results='asis'}
ARI <- apply(results.all, 2, adjustedRandIndex,
             x = substring(rownames(results.all), first = 18)) %>% 
  sort(decreasing = T) %>% 
  data.frame(ARI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, ARI)
pander::pandoc.table(ARI, justify = "lc")
```

## Mutual Information

The larger the better.

```{r MI, results='asis'}
MI <- apply(results.all, 2, mutinformation,
            Y = substring(rownames(results.all), first = 18)) %>% 
  sort(decreasing = T) %>% 
  data.frame(MI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, MI)
pander::pandoc.table(MI, justify = "lc")
```

The mutual information for the meta consensus clustering is `r entropy::mi.plugin(final.compare)`.

## Cumulative Distribution Function

```{r CDF}
CDF.each <- list(nmfDiv = results.NMF$nmfDiv$consensusMatrix,
                 nmfEucl = results.NMF$nmfEucl$consensusMatrix) %>%
  ldply(.fun = function(x) x[lower.tri(x)]) %>%
  rbind(., ldply(results.CCP, function(x)
    x[[4]]$consensusMatrix[lower.tri(x[[4]]$consensusMatrix)])) %>%
  as.data.frame %>%
  set_rownames(.$.id) %>%
  select(-.id) %>%
  t %>%
  as.data.frame %>%
  gather(key = Method, value = CDF, 1:ncol(.))

ggplot(CDF.each, aes(x = CDF, colour = Method)) +
  stat_ecdf() +
  facet_wrap(~ Method) +
  theme(legend.position = "none")
```

## Proportion of Ambiguous Clusters

The lower the better.

```{r PAC, results='asis'}
PAC <- results.CCP %>%
  sapply(., function(x) PAC(x[[4]]$consensusMatrix)) %>%
  c(nmfDiv = PAC(results.NMF$nmfDiv$consensusMatrix),
    nmfEucl = PAC(results.NMF$nmfEucl$consensusMatrix)) %>%
  set_names(ALG.NAMES) %>% 
  sort %>% 
  data.frame(PAC = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, PAC)
pander::pandoc.table(PAC, justify = "lc")
```

The PAC for the meta consensus matrix is `r PAC(meta.cm)`.

# Internal Evaluation

## Davies-Bouldin Index

For DBI, the lower the better.

```{r DBI, results='asis'}
DBI <- dat %>%
  apply(results.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average") %>%
  sort %>% 
  data.frame(DBI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, DBI)
pander::pandoc.table(DBI, justify = "lc")
```

## Dunn Index

For DI, the larger the better.

```{r DI, results='asis'}
DI <- dat %>%
  apply(results.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Dunn, intracls = "average", intercls = "average") %>%
  sort(decreasing = T) %>% 
  data.frame(DI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, DI)
pander::pandoc.table(DI, justify = "lc")
```

## Silhouette Average Width

For SAW, the larger the better.

```{r SAW, results='asis'}
SAW <- apply(results.all, 2, function(x) summary(silhouette(x, dist(dat)))$avg.width) %>%
  sort(decreasing = T) %>% 
  data.frame(SAW = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, SAW)
pander::pandoc.table(SAW, justify = "lc")
```

## Rousseeuw's Silhouette

For Rousseuw's Silhouette internal cluster quality index (RS), the larger the better.

```{r RS, results='asis'}
RS <- apply(results.all, 2, index.S, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(RS = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, RS)
pander::pandoc.table(RS, justify = "lc")
```

## C-Index

For CI, the lower the better.

```{r CI, results='asis'}
CI <- apply(results.all, 2, index.G3, d = dist(dat)) %>%
  sort %>% 
  data.frame(CI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, CI)
pander::pandoc.table(CI, justify = "lc")
```

## Baker and Hubert Index

For BHI, the larger the better.

```{r BHI, results='asis'}
BHI <- apply(results.all, 2, index.G2, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(BHI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, BHI)
pander::pandoc.table(BHI, justify = "lc")
```

## Calinski-Harabasz Index

For CHI, the larger the better.

```{r CHI, results='asis'}
CHI <- apply(results.all, 2, index.G1, x = dat) %>%
  sort(decreasing = T) %>% 
  data.frame(CHI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, CHI)
pander::pandoc.table(CHI, justify = "lc")
```

## Summary

Here is a summary of all the indices for each algorithm, in unsorted order.

```{r index_summary, results='asis'}
index.summary <- Reduce(merge, list(ARI, MI, PAC, DBI, DI, SAW, RS, CI, BHI, CHI))
pander::pandoc.table(index.summary,
                     justify = paste0("l",
                                      paste(rep("c", ncol(index.summary) - 1),
                                            collapse = "")))
```

# Ranked Indices

The table below shows the ranking of algorithms for performance on a clustering index, for each index. There is an additional column that shows the propoportion of indices where an algorithm was ranked **first or second**.

```{r ranked_indices, results='asis'}
ranked.indices <- data.frame(Algorithms = index.summary$Algorithms,
                             ARI = rank(-index.summary$ARI),
                             MI = rank(-index.summary$MI),
                             PAC = rank(index.summary$PAC),
                             DBI = rank(index.summary$DBI),
                             DI = rank(-index.summary$DI),
                             SAW = rank(-index.summary$SAW),
                             RS = rank(-index.summary$RS),
                             CI = rank(index.summary$CI),
                             BHI = rank(-index.summary$BHI),
                             CHI = rank(-index.summary$CHI)) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
  
pander::pandoc.table(ranked.indices,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.indices) - 1),
                                            collapse = "")))
```

If we were to conduct a meta consensus clustering, a weight for each algorithm needs to be assigned. One such way of doing so is using the inverse rank sums. We can sum the ranks for each algorithm, and assign higher weight for consistently high ranked methods (1st or 2nd) and vice versa. This is shown below:

```{r inverse_rank_sum, results='asis'}
ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 12)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)

pander::pandoc.table(ranked.sums,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.sums) - 1),
                                            collapse = "")))
```

# Simulations

To confirm the clustering results from using the TCGA dataset, we need to try out the algorithms on a few simulated datasets, designed to test the robustness of each method. The `clusterSim` package provides very good built-in examples to use.

## Worms Dataset

The following two-cluster dataset is our first simulation:

```{r worms_plot}
set.seed(1)
sw <- shapes.worms()
dat <- sw$data
true.class <- sw$clusters
plot(sw$data, col = c(2:3)[true.class], xlab = "x", ylab = "y",
     main = "Two clusters with atypical parabolic shapes (worms)")
```

```{r worms_index}
# combine results
worms.CCP <- readRDS("Worms/outputs/results_CCP.rds")
worms.NMF <- readRDS("Worms/outputs/results_NMF.rds")
worms.all <- worms.CCP %>%
  sapply(., function(x) x[[2]]$consensusClass) %>%
  cbind(., worms.NMF$nmfDiv$consensusClass,
        worms.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# indices
ARI <- apply(worms.all, 2, adjustedRandIndex, x = true.class)
MI <- apply(worms.all, 2, mutinformation, Y = true.class)
PAC <- worms.CCP %>%
  sapply(., function(x) PAC(x[[2]]$consensusMatrix)) %>%
  c(nmfDiv = PAC(worms.NMF$nmfDiv$consensusMatrix),
    nmfEucl = PAC(worms.NMF$nmfEucl$consensusMatrix)) %>% 
  set_names(ALG.NAMES)
DBI <- dat %>%
  apply(worms.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average")
DI <- dat %>%
  apply(worms.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Dunn, intracls = "average", intercls = "average")
SAW <- apply(worms.all, 2, function(x) summary(silhouette(x, dist(dat)))$avg.width)
RS <- apply(worms.all, 2, index.S, d = dist(dat))
CI <- apply(worms.all, 2, index.G3, d = dist(dat))
BHI <- apply(worms.all, 2, index.G2, d = dist(dat))
CHI <- apply(worms.all, 2, index.G1, x = dat)
```

The ranked indices and weights tables are shown below:

```{r worms_summary, results='asis'}
ranked.indices <- data.frame(Algorithms = ALG.NAMES,
                             ARI = rank(-ARI, ties.method = "min"),
                             MI = rank(-MI, ties.method = "min"),
                             PAC = rank(PAC, ties.method = "min"),
                             DBI = rank(DBI, ties.method = "min"),
                             DI = rank(-DI, ties.method = "min"),
                             SAW = rank(-SAW, ties.method = "min"),
                             RS = rank(-RS, ties.method = "min"),
                             CI = rank(CI, ties.method = "min"),
                             BHI = rank(-BHI, ties.method = "min"),
                             CHI = rank(-CHI, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
pandoc.table(ranked.indices,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.indices) - 1),
                                            collapse = "")))

ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 12)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  dplyr::select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)
pandoc.table(ranked.sums,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.sums) - 1),
                                            collapse = "")))
```
