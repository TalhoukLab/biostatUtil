---
title: "Performance Evaluation of Consensus Clustering Algorithms"
subtitle: "STAT 598 Progress Report"
author: "Derek Chiu"
date: "September 16, 2015"
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 3
---

# Preface
This progress report fulfills the UBC Science Co-op requirement to submit a work term report at the end of every four month period. BC Cancer Agency (BCCA) is a not-for-profit organization that aims to provide care for cancer patients and conduct innovative cancer research. Our department, OvCaRe, is the Ovarian Cancer Research team tasked with studying ovarian cancers of many types. The objective of the project I am working on is to discover a viable classifier for ovarian high-grade serous carcinoma (HGSC). My role is to help devise a clustering algorithm that can partition tumour samples of HGSC into different subtypes without knowledge of the underlying pathological properties of each sample. Instead, only gene expression data will be used in the prediction. The progress report will evaluate the method we are using, consensus clustering, on a publicly available data set as well as some simulated data sets. The final technical report will contain results of our method applied on HGSC data from our own cohort.

```{r setup, echo=FALSE, cache=FALSE, include=TRUE, message=FALSE}
# packages
library(tcgaHGSC)
library(ConClust)
library(clv)
library(clValid)
library(clusterSim)
library(NMF)
library(mclust)
library(clue)
library(gplots)
library(ggplot2)
library(RColorBrewer)
library(xtable)
library(pander)
library(plyr)
library(dplyr)
library(magrittr)
library(knitr)
library(stringr)
library(rgl)
library(COMMUNAL)
library(pamr)
panderOptions("table.split.table", 300)  # don't split any tables
opts_chunk$set(echo = FALSE)  # hide all code from output
# names of clustering algorithms
ALG.NAMES <- c("HC (A. Euc)", "HC (S. Euc)", "HC (Diana)",
               "KM (Euc)", "KM (Spear)", "KM (MI)",
               "PAM (Euc)", "PAM (Spear)", "PAM (MI)",
               "NMF (Div)", "NMF (Euc)")
```

# Introduction
Unsupervised learning is the process of inferring something about a data structure without knowing its true class labels. Cluster analysis is an unsupervised learning method of assigning entities into different groups based on one or more of their attributes. It is unsupervised because we do not know the true partitions of the entities. The goal is to place similar objects together in the same cluster and separate dissimilar objects into different clusters. For example, in genomics studies, we frequently try and cluster patient samples measured on a large number of molecular features.

When we obtain a clustering assignment from an algorithm, we often want to evaluate its performance and validity. Ideally, a good clustering algorithm is able to differentiate entities without knowledge of the true class labels. In addition, we want the algorithm to arrive at a stable clustering result. Some algorithms are sensitive to initial conditions and we do not want the assignments to be dependent on those. Finally, the choice of the number of clusters is not trivial in unsupervised explorations. This will not be a problem in simulations because we do know the true class labels. However, it is important to keep in mind that for real data the number of clusters should be determined using the data structure.

# Methods

## Clustering Algorithms
There are many clustering algorithms, each approaching the problem in a different way. It is important to note the advantages and limitations of each algorithm. These are some definitions of clustering performance^18^:

- **Compactness**: how close together or tight objects are to each other within a cluster
- **Separation**: how far apart objects are in different clusters
- **Connectivity**: how connected the objects are to their closest neighbours

### Hierarchical Clustering
Hierarchical clustering (HC) is popular because of its intuitive representation using dendrograms (trees). More similar objects are joined near the bottom of a dendrogram whereas less similar objects are joined at a higher tree height. First a distance metric is used to compute a distance matrix. Then objects/features are clustered based on a linkage type. The linkage criterion determines the distances among a set of objects/features using the pairwise distances. For example, an average linkage would use the average pairwise distances. On the other hand, single linkage uses the minimum pairwise distance. A dendrogram with all objects/features can be made by recursively linking increasingly larger subsets of observations together. 

Single linkage works well for data sets exhibiting connectivity but not compactness. This is because single linkage looks for minimum pairwise distances, which would cluster together neighbouring points. An example of this would be tree rings. The clusters are circles, and objects that are far away can be in the same cluster compared to objects that are actually closer. On the other hand, average linkage works well for data sets exhibiting compactness. This works where the clusters look like non-overlapping blobs.

### K-Means and PAM
First, k means (centroids) are randomly initialized in the multidimensional object/feature space that we wish to cluster. The clusters are formed by assigning each object/feature to its closest centroid. The centroids are recalculated based on the cluster memberships and the  subsequently updated. This procedure is iterated until the centroids converge.

There are two caveats to note when using k-means. First, sometimes the cluster assignments are unstable because they depend on the random initialization of the centroids. We preferably want to repeat the algorithm many times to see whether the clusters are sensitive to the choice of initial centroid. Secondly, choosing k is not arbitrary. Cross-validation using an appropriate loss function is a popular method for choosing k. Other methods use evaluation indices, some of which we will describe later in the report.

Partitioning Around Medoids (PAM) is very similar to k-means except that we randomly initialize k random data points (medoids). Medoids must be actual data points whereas centroids can be any point in the feature space.

### Nonnegative Matrix Factorization (NMF)
Given a non-negative data matrix $A$, we can factor it into two matrices $W$ and $H$, which are also non negative. $W$ and $H$ have important properties. Suppose $A$ has genes as rows and samples as columns. If we are interested in clustering samples, then $H$ has a reduced gene space of metagenes that fully explain the samples. Samples are clustered based on the metagene they are most associated with. If we are interested in clustering genes, then $W$ has a reduced sample space of metasamples that fully explain the genes. Genes are clustered based on the metasample they are most associated with.

In gene expression data, it is common to standardize the genes. Doing so would likely disrupt the nonnegativity of $A$ required for NMF. A simple remedy can solve this problem: append the matrix $-A$ to the bottom of $A$ (preserving the same number of columns), and set all negative entries to 0. The computational complexity will have been doubled as a result. NMF takes a long time to run, but studies have shown clustering assignments can be highly stable^1^.

## Consensus Clustering
Monti et al.^2^ describe consensus clustering as an algorithm but also as a method of combining realizations of other clustering algorithms. Algorithms like k-means and PAM are unstable, as the clustering assignments depend on the initialization of centroids and medoids, respectively. Consensus clustering combines results from repeated runs of a clustering algorithm. Typically, each run uses different random subsamples of the data to model sampling variability. A final clustering assignment is determined based on agreement across replicates.

The consensus matrix is a significant aspect of consensus clustering. Consider a consensus matrix $C$. Entry $C_{ij}$ is the proportion of runs that object $i$ and object $j$ were clustered together, out of all runs in which $i$ and $j$ were both selected in subsamples, where $1\leq i,j\leq N$, $N$ = number of objects. $C$ is symmetric because $C_{ij} = C_{ji}$ (i.e. no order exists). All entries range from 0 to 1 since $C_{ij}$ are proportions. A perfect clustering would consist of a consensus matrix with only 0 (objects never clustered together) or 1 (objects always clustered together). The final step to obtaining the consensus clustering assignment is to use hierarchical clustering on the consensus matrix.

Based on the hierarchical clustering, we can plot a heatmap of the consensus matrix. Repeating this for different values of k (number of clusters) and looking at the corresponding heatmaps, we can determine which value of k provides the most stable clustering assignment. The goal is to have a well-defined diagonal block structure in the heatmap, one block per cluster. Using a performance measure such as PAC (which we will describe) would be a more formal method of assessment that doesn't rely on potentially subjective visualizations.

Consensus clustering attempts to adjust for the randomness introduced by subsampling and the clustering algorithm itself. An extension is sometimes called meta-consensus clustering, where we aggregate results across different algorithms in addition to aggregating across subsamples within an algorithm. For instance, we may combine the consensus clustering assignments from 10 different algorithms to come up with a final clustering. The choice of which algorithms to use is not trivial. We hope to prove that using meta-consensus clustering is superior to using consensus clustering of one algorithm.

# Performance Evaluation: TCGA Dataset

```{r tcga_load}
# load results and data
tcga.CCP <- readRDS("TCGA/outputs/results_CCP.rds")
tcga.NMF <- readRDS("TCGA/outputs/results_NMF.rds")
data(hgsc)
dat <- hgsc %>%
  set_rownames(.$UNIQID) %>%
  dplyr::select(which(sapply(., class) == "numeric")) %>%
  extract(apply(., 1, sd) > 1, ) %>%
  t %>%
  scale

# true number of clusters
k <- 4
```

A published dataset from TCGA^3^ uses `r nrow(hgsc)` genes to cluster `r ncol(hgsc) - 1` HGSC samples into the four subtypes: *mesenchymal*, *immunoreactive*, *differentiated*, and *proliferative*. TCGA used consensus clustering with NMF. In this report, we consider the following clustering algorithms to use in consensus clustering. Abbreviated names will be used in the report and are shown in bold.

* Hierarchical Clustering with Euclidean distance
    - Average Linkage: **HC (A. Euc)**
    - Single Linkage: **HC (S. Euc)**
    - DIvisive ANAlysis: **HC (Diana)**
* K-Means
    - Euclidean distance: **KM (Euc)**
    - Spearman distance: **KM (Spear)**
    - Mutual Information distance: **KM (MI)**
* PAM
    - Euclidean distance: **PAM (Euc)**
    - Spearman distance: **PAM (Spear)**
    - Mutual Information distance: **PAM (MI)**
* NMF
    - KL divergence: **NMF (Div)**
    - Euclidean distance: **NMF (Euc)**
  
The number of repetitions used for consensus clustering is 1000. Each subsample (replicate) of the data uses 80% of the total number of features.

The figure below shows the meta-consensus matrix across the `r length(ALG.NAMES)` algorithms, each of which used consensus clustering.
 
```{r tcga_heatmap}
# Combine class labels from CCP and NMF
tcga.all <- tcga.CCP %>%
  sapply(., function(x) x[[k]]$consensusClass) %>%
  cbind(., tcga.NMF$nmfDiv$consensusClass,
        tcga.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# TCGA classes
true.class <- as.factor(substring(rownames(tcga.all), first = 18))

# Meta-consensus matrix heatmap
meta.cm <- consensusMatrix(tcga.all)
palOrRd <- colorRampPalette(brewer.pal(n = 9, "OrRd"))(256)
heatmap.2(meta.cm, labRow = NA, labCol = NA, col = palOrRd,
          trace = "none", density.info = "none")
```

From the heatmap, we do not see a very high concordance across algorithms. For example, the proportion of cases with at least 0.6 agreement (orange colour) is only `r round(((sum(meta.cm >= 0.6) - ncol(meta.cm)) / 2) / choose(ncol(meta.cm), 2), 3)`. There is some evidence of a four-class data structure.

The confusion matrix is shown below for the meta consensus classes compared to TCGA's classification. Several metrics are shown for each class.

```{r tcga_conf_mat, results='asis'}
final.compare <- hclust(dist(meta.cm), method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., TCGA = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table

cm <- caret::confusionMatrix(final.compare)
print(xtable(format(ftable(cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

```{r tcga_conf_mat_stats, results='asis'}
pandoc.table(cm$byClass, digits = 3)
```

We hope to use cluster evaluation indices to give more weight to better-performing algorithms in the construction of the meta-consensus clusters. Here we outline two main types of evaluation measures: external evaluation indices and internal evaluation indices.

## External Evaluation
External evaluation refers to the situation when we compare our clustering assignments to true class labels, gold standards, or reference labels. In the TCGA analysis, we use their published clustering result as reference labels. The downside of using external evaluation is that the reference classes may not be correctly clustered themselves, and we are treating these as the norm. None the less, we can explore a few metrics to see how our own clustering performance compares to that of TCGA.

We expect that our own NMF-based algorithms will perform well on these evaluation indices because the reference labels were clustered using NMF too.

### Purity and Entropy

Purity is defined as the sum of the entities of maximal class in each cluster divided by the total number of entities^4^. The equation is:
\begin{center}
$Purity = \dfrac{1}{n}\sum\limits_{r = 1}^{k}\max\limits_{i}(n_r^i)$
\end{center}
where $n$ is the total number of entities, $k$ is the number of clusters, $i$ is a particular class, and $n_r^i$ is the number of objects classified into class $i$ in cluster $r$. The larger the purity, the better the clustering accuracy.
```{r purity, results='asis'}
Purity <- apply(tcga.all, 2, function(x) purity(as.factor(x), true.class)) %>%
  sort(decreasing = T) %>% 
  data.frame(Purity = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, Purity)
pandoc.table(Purity, justify = "lc")
```

Entropy measures the amount of uncertainty in each cluster^4^. The equation is:
\begin{center}
$Entropy = -\dfrac{1}{n\log{q}}\sum\limits_{r = 1}^{k}\sum\limits_{i = 1}^{q}n_r^i\log{\dfrac{n_r^i}{n_r}}$
\end{center}
where $n$, $k$, $i$, and $n_r^i$ are same as above, and $q$ is the number of classes.
The smaller the entropy, the less uncertain we are of the cluster membership, and the better the clustering performance.
```{r entropy, results='asis'}
Entropy <- apply(tcga.all, 2, function(x) entropy(as.factor(x), true.class)) %>% 
  sort %>% 
  data.frame(Entropy = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, Entropy)
pandoc.table(Entropy, justify = "lc")
```

Not surprisingly, we see that the two NMF-based algorithms score the highest on both purity and entropy.

### Kappa Statistics

Cohen's kappa statistic $\kappa$ measures the level of agreement between two raters.^5^ In our case, the raters are different clustering algorithms. The equation is shown below:
\begin{center}
$\kappa = \dfrac{p_o - p_e}{1 - p_e}$
\end{center}
where $p_o$ is the proportion of entities agreed upon and $p_e$ is the proportion of entities expected to agree by chance.

The weighted $\kappa$ statistic is as follows and takes into account the off-diagonal elements of the confusion matrix between the two raters.^6^ In other words, the disagreements between the raters are weighed in this formulation:
\begin{center}
$\kappa_w = 1 - \dfrac{\sum_{i=1}^k\sum_{j=1}^k w_{ij}o_{ij}}{\sum_{i=1}^k\sum_{j=1}^k w_{ij}e_{ij}}$
\end{center}
where $k$ is the number of classes, and $w_{ij}$, $o_{ij}$, and $e_{ij}$ are the weight, observed, and expected matrices respectively.

Fleiss provided the following rating scheme for $\kappa$.^7^ Albeit arbitrary and not numerically determined it still serves as a rough guideline.
```{r kappa_ratings, results='asis'}
kappa_rating <- data.frame(Rating = c("Poor", "Fair", "Excellent"),
                           Kappa = c("Less than 0.40", "Between 0.40 and 0.75", "Greater than 0.75"))
pandoc.table(kappa_rating)
```

The results for the TCGA dataset are shown below. The ratings are based on the weighted $\kappa$:

```{r kappa_results, results='asis'}
kappas <- alply(tcga.all, 2, function(x)
  table(x, true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  sapply(., psych::wkappa) %>% 
  as.data.frame %>% 
  mutate(Meta = psych::wkappa(final.compare)) %>% 
  t %>% 
  apply(., 2, unlist) %>% 
  as.data.frame %>%
  set_colnames(c("K", "wt.K")) %>% 
  mutate(Algorithm = rownames(.),
         Rating = ifelse(wt.K < 0.4, "Poor",
                         ifelse(wt.K > 0.75, "Excellent",
                                "Fair"))) %>%
  arrange(desc(wt.K)) %>% 
  dplyr::select(Algorithm, K, wt.K, Rating) %>% 
  rename(Kappa = K, "Weighted Kappa" = wt.K)

pandoc.table(kappas)
```

### Adjusted Rand Index

The Rand Index measures agreement between two classes by counting the number of pairs of objects that are clustered together in two different clustering assignments.^8^ Hubert & Araibe proposed an adjusted version to account for random chance.^9^ The equation for the adjusted Rand index (ARI) is shown below:^8,9,10^

\begin{center}
$ARI = \dfrac{\sum_{ij}\binom{n_{ij}}{2} - [\sum_i\binom{a_i}{2}\sum_j\binom{b_j}{2}] / \binom{n}{2}}{\frac{1}{2}[\sum_i\binom{a_i}{2} + \sum_j\binom{b_j}{2}] - [\sum_i\binom{a_i}{2}\sum_j\binom{b_j}{2}] / \binom{n}{2}}$
\end{center}

where $n_{ij}$'s are entries in the cluster assignment confusion matrix, $a_i$ and $b_j$ are row and column marginal totals respectively, and $n$ is the grand total. The ARI is 1 when the two clusterings are perfect, and 0 when there is no concordance.

```{r ARI, results='asis'}
ARI <- apply(tcga.all, 2, adjustedRandIndex, x = true.class) %>% 
  sort(decreasing = T) %>% 
  data.frame(ARI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, ARI)
pandoc.table(ARI, justify = "lc")
```

## Internal Evaluation
Interval evaluation assesses how well clustered objects are based on the features they are measured on in the data. Ideally, we want objects in the same cluster to be close together and objects in different clusters to be far from each other. Different definitions of distance define compactness and separation differently. Some evaluation indices combine these distances into a unitless measure that we can use to compare clustering results.

We will rank each algorithm on the indices using the minimum rank to break ties. For example, if two algorithms are tied for third, the next best algorithm is ranked fifth. This method is commonly used in sports.

### Proportion of Ambiguously Clustered Pairs (PAC)

The notion of PAC is closely related to consensus clustering. Senbabaoglu et al. argue that PAC is a better measure of determining the optimal number of clusters from consensus matrices, compared to other measures such as the Gap statistic, CLEST, etc.^11^ The idea is simple: the proportion of ambiguously clustered pairs is the number of entries in the consensus matrix that are not 0 or 1. Recall that the consensus matrix is symmetric, so we only need to consider the lower (or upper) triangular matrix. In most applications, the definition of ambiguity is less stringent, and any entry greater than $p$ or less than $1 - p$ contributes to PAC, where $p$ is a small proportion (e.g. 0.05).

Although Senbabaoglu et al. initially used PAC to determine the optimal number of clusters, here we are using it to compare different clustering algorithms. None the less, we want the PAC to be as small as possible. A perfect score for PAC would be 0, meaning that all entries in the corresponding consensus matrix are either 0 or 1. The advantage of using this index is that it utilizes results from consensus clustering and thus not biased towards a particular distance metric. However, a major limitation is that PAC only uses stability evidence and doesn't assess accuracy. A clustering algorithm can be consistently wrong and do well for PAC, masking the apparent performance of said algorithm to make correct classifications.

Here we use 0.05 and 0.95 as the bounds for PAC classification.

```{r PAC, results='asis'}
PAC <- tcga.CCP %>%
  sapply(., function(x) PAC(x[[k]]$consensusMatrix, 0.05, 0.95)) %>%
  c(nmfDiv = PAC(tcga.NMF$nmfDiv$consensusMatrix, 0.05, 0.95),
    nmfEucl = PAC(tcga.NMF$nmfEucl$consensusMatrix, 0.05, 0.95)) %>%
  set_names(ALG.NAMES) %>% 
  sort %>% 
  data.frame(PAC = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, PAC)
pandoc.table(PAC, justify = "lc")
```

A related figure is the cumulative distribution function (CDF) for each consensus matrix. Each panel corresponds to one algorithm. Recall that only the lower triangular entries of each consensus matrix are graphed. The ideal CDF would look like a horizontal line between 0 and 1, and straight at 0 and 1, because this would correspond to PAC = 0.
 
```{r CDF}
CDF.each <- ldply(tcga.CCP, function(x)
  x[[k]]$consensusMatrix[lower.tri(x[[k]]$consensusMatrix)]) %>% 
  rbind(., ldply(list(tcga.NMF$nmfDiv$consensusMatrix,
                      tcga.NMF$nmfEucl$consensusMatrix),
                 function(x) x[lower.tri(x)])) %>%
  t %>% 
  as.data.frame %>%
  set_names(ALG.NAMES) %>% 
  tidyr::gather(key = Method, value = CDF, 1:ncol(.))

ggplot(CDF.each, aes(x = CDF, colour = Method)) +
  stat_ecdf() +
  facet_wrap(~ Method) +
  labs(y = "Prob", title = "Cumulative Distribution Function for each Consensus Matrix") +
  theme(legend.position = "none")
```

### Davies-Bouldin Index

The Davies-Bouldin Index (DBI) measures the ratio of the within cluster scatter to the between cluster separation^12^. Hence, we want to *minimize* DBI such that objects in the same cluster are not too scattered and objects in different clusters are well separated. The advantage of using DBI to compare algorithms is that it utilizes properties of the data structure, and not true class labels. However, like the PAC, it may not indicate we are making the correct partitions.

```{r DBI, results='asis'}
DBI <- dat %>%
  apply(tcga.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average") %>%
  sort %>% 
  data.frame(DBI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, DBI)
pandoc.table(DBI, justify = "lc")
```

### Dunn Index

The Dunn Index (DI) is a ratio of the minimum intercluster distance to the maximum intracluster distance^13^. In this report, we use the following definitions for the cluster-specific distances:

- Intercluster distance: distance between two farthest points in different clusters
- Intracluster distance: distance between two closest points in the same cluster

As a result, the higher the DI the better the clustering assignment. Similar to the DBI, the DI uses the data itself to determine clustering performance. Other definitions of the intercluster and intracluster distances exist.

```{r DI, results='asis'}
DI <- apply(tcga.all, 2, dunn, distance = dist(dat)) %>% 
  sort(decreasing = T) %>% 
  data.frame(DI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, DI)
pandoc.table(DI, justify = "lc")
```

### Rousseeuw's Silhouette

Rousseuw's Silhouette internal cluster quality index (RS) measures how well each object is clustered by comparing its dissimilarity with other points in its own cluster to points in its neighbouring cluster^14^. The silhouette index ranges from -1 to 1. If the index is close to 1 then the object is clustered well and if it is close to -1 then the object would be better clustered in the neighbouring cluster.

The average silhouette index measures how well *all* objects are clustered, and is the measure we use here to compare the different algorithms. Just like the individual silhouette indices, we want to maximize the RS as well.

```{r RS, results='asis'}
RS <- apply(tcga.all, 2, index.S, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(RS = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, RS)
pandoc.table(RS, justify = "lc")
```

### C-Index

The C-Index (CI) as proposed by Hubert & Levin is a ratio of within-cluster dissimilarities^15^. For the CI, minimizing it indicates the optimal number of clusters. Here we use it to compare algorithms.

```{r CI, results='asis'}
CI <- apply(tcga.all, 2, index.G3, d = dist(dat)) %>%
  sort %>% 
  data.frame(CI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, CI)
pandoc.table(CI, justify = "lc")
```

### Gamma Index

The Gamma Index described by Baker & Hubert is a ratio of the difference in the number of discordant comparisons to concordant comparisons, over all comparisons^16^. A high value for GI would thus indicate the clustering assignments have more concordance than discordance. Originally used to determine the optimal number of clusters, here we use it to compare algorithms.

```{r GI, results='asis'}
GI <- apply(tcga.all, 2, index.G2, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(GI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, GI)
pandoc.table(GI, justify = "lc")
```

### CH Index

The Calinski-Harabasz pseudo F-statistic index (CHI) measures the ratio of the between-group dispersion matrix to the within-group dispersion matrix, each normalized by their respective degrees of freedom^17^. The CHI is called the pseudo F-statistic because the dfs are similar to the ANOVA F-statistic. A maximal CHI indicates the optimal number of clusters, yet here we use it to compare algorithms.

An advantage of the CHI over other indices is that it does not depend on a distance metric (e.g. Euclidean) in its formulation. Algorithms that use Euclidean distances would perform favorably for evaluation indices that use a Euclidean distance matrix as an input.

```{r CHI, results='asis'}
CHI <- apply(tcga.all, 2, index.G1, x = dat) %>%
  sort(decreasing = T) %>% 
  data.frame(CHI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, CHI)
pandoc.table(CHI, justify = "lc")
```

### Connectivity

Connectivity measures how connected the clusters are based on its nearest neighbours^18^. The measure ranges from 0 to infinity and a small value indicates high connectivity.

```{r Connectivity, results='asis'}
Conn <- apply(tcga.all, 2, clValid::connectivity, distance = dist(dat)) %>% 
  sort %>% 
  data.frame(Conn = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, Conn)
pandoc.table(Conn, justify = "lc")
```

### Summary

Here is a summary of all the evaluation indices for each algorithm, in no particular order.

```{r index_summary, results='asis'}
index.summary <- Reduce(merge, list(PAC, DBI, DI, RS, CI, GI, CHI, Conn))
pandoc.table(index.summary,
             justify = paste0("l",
                              paste(rep("c", ncol(index.summary) - 1),
                                    collapse = "")))
```

## Weighted Assessment

### All Indices
The table below shows the ranking of algorithms for clustering performance on each internal evaluation index. There is an additional column that shows the proportion of indices where an algorithm was ranked **first or second**.

```{r ranked_indices, results='asis'}
ranked.indices <- data.frame(Algorithms = index.summary$Algorithms,
                             PAC = rank(index.summary$PAC),
                             DBI = rank(index.summary$DBI),
                             DI = rank(-index.summary$DI),
                             RS = rank(-index.summary$RS),
                             CI = rank(index.summary$CI),
                             GI = rank(-index.summary$GI),
                             CHI = rank(-index.summary$CHI),
                             Conn = rank(index.summary$Conn)) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
  
pandoc.table(ranked.indices,
             justify = paste0("l",
                              paste(rep("c", ncol(ranked.indices) - 1),
                                    collapse = "")))
```

If we were to conduct a weighted meta-consensus clustering, a weight for each algorithm needs to be assigned. One such way of doing so is using the inverse rank sums based on the indices. We can sum the ranks for each algorithm, and assign higher weight for consistently higher ranked methods (low sum of ranks). This is shown in the table below:

```{r inverse_rank_sum, results='asis'}
ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 10)], 1, sum),
         Weight = Sum^-1 / sum(Sum^-1)) %>% 
  dplyr::select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)

pandoc.table(ranked.sums,
             justify = paste0("l", paste(rep("c", ncol(ranked.sums) - 1),
                                         collapse = "")))
```

### PAC and CHI only
There is a major issue with the way we ranked the indices in the previous section. Upon closer inspection, the algorithm that uses hierarchical clustering with single linkage and Euclidean distance actually performs terrible in the clustering analysis. The table below shows the number of samples clustered into each of the four clusters using HC (S. Euc):

```{r hcSEucl_bad, results='asis'}
pandoc.table(table(tcga.all[, 2]))
```

Clearly, HC (S. Euc) fails miserably at the clustering task, and yet the algorithm performs very well for several indices. One suggestion is to consider only PAC and CHI, because they do not depend on a specific distance metric. We also opt to use the index scores themselves to determine weights instead of ranks. One reason is that since we only have two indices, we collapse too much information by using ranks. Secondly, we want to be more precise in how much weight to assign.

```{r PAC_CHI_only, results='asis'}
PAC.CHI.only <- index.summary %>% 
  dplyr::select(Algorithms, PAC, CHI) %>% 
  mutate(Weight.PAC = (1 - PAC) / sum((1 - PAC)),
         Weight.CHI = CHI / sum(CHI),
         Weight = (Weight.PAC + Weight.CHI) / 2) %>% 
  arrange(desc(Weight)) %>% 
  dplyr::select(-Weight.PAC, -Weight.CHI)

pandoc.table(PAC.CHI.only, justify = "lccc")
```

Below we show the weighted meta-consensus matrix using the weights from the previous table:

```{r tcga_weighted_meta_cm}
weights <- PAC.CHI.only[order(match(PAC.CHI.only$Algorithms, colnames(tcga.all))), "Weight"]
weighted.meta.cm <- consensusMatrix(tcga.all, weights)
heatmap.2(weighted.meta.cm, labRow = NA, labCol = NA, col = palOrRd,
          trace = "none", density.info = "none",
          hclustfun = function(x) hclust(x, method = "average"))
```

And the corresponding confusion matrix with TCGA's clusters:

```{r tcga_weighted_conf_mat, results='asis'}
weighted.final.compare <- hclust(dist(weighted.meta.cm), method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., TCGA = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table

cm <- caret::confusionMatrix(weighted.final.compare)
print(xtable(format(ftable(cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

## Comparison of Accuracy
We can now compare the **accuracy** of all `r length(ALG.NAMES) + 2` clustering results: `r length(ALG.NAMES)` individual clustering algorithms plus the unweighted and weighted meta-consensus clustering algorithms.

Accuracy is defined as the number of correct clusterings out of the total number of clusterings. In our case, we consider the *truth* to be the reference clustering from TCGA. It can be calculated from a confusion matrix by dividing the sum of the diagonal by the sum of all entries.

```{r tcga_accuracy, results='asis'}
Accuracy <- alply(tcga.all, 2, function(x)
  table(Meta = x, TCGA = true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  c(list(Meta = final.compare,
         Meta.Weighted = weighted.final.compare)) %>% 
  sapply(., function(x) sum(diag(x)) / sum(x)) %>% 
  data.frame(Accuracy = .) %>% 
  mutate(Algorithm = rownames(.)) %>% 
  dplyr::select(Algorithm, Accuracy) %>% 
  arrange(desc(Accuracy))

pandoc.table(Accuracy, justify = "lc")
```

The NMF-based algorithm using KL divergence clustering results has the highest number of commonly clustered samples with the TCGA clustering.

# Simulations

To confirm the clustering results from using the TCGA dataset, we need to try the algorithms on a few simulated datasets, designed to test the robustness of each method. The `clusterSim` package provides very good built-in examples to use.

## Worms Dataset

The following two-cluster dataset is our first simulation:

```{r worms_plot}
set.seed(1)
sw <- shapes.worms()
dat <- sw$data
true.class <- sw$clusters
k <- 2
plot(dat, col = c(2:3)[true.class], xlab = "x", ylab = "y",
     main = "Two clusters with atypical parabolic shapes (worms)")
```

### Indices

```{r worms_index}
# combine results
worms.CCP <- readRDS("Worms/outputs/results_CCP.rds")
worms.NMF <- readRDS("Worms/outputs/results_NMF.rds")
worms.all <- worms.CCP %>%
  sapply(., function(x) x[[k]]$consensusClass) %>%
  cbind(., worms.NMF$nmfDiv$consensusClass,
        worms.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# indices
index.summary <- data.frame(
  Algorithms = ALG.NAMES,
  PAC = worms.CCP %>%
    sapply(., function(x) PAC(x[[k]]$consensusMatrix, 0.05, 0.95)) %>%
    c(nmfDiv = PAC(worms.NMF$nmfDiv$consensusMatrix, 0.05, 0.95),
      nmfEucl = PAC(worms.NMF$nmfEucl$consensusMatrix, 0.05, 0.95)) %>% 
    set_names(ALG.NAMES),
  DBI = dat %>%
    apply(worms.all, 2, cls.scatt.data, data = .) %>%
    sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average"),
  DI = apply(worms.all, 2, dunn, distance = dist(dat)),
  RS = apply(worms.all, 2, index.S, d = dist(dat)),
  CI = apply(worms.all, 2, index.G3, d = dist(dat)),
  GI = apply(worms.all, 2, index.G2, d = dist(dat)),
  CHI = apply(worms.all, 2, index.G1, x = dat),
  Conn = apply(worms.all, 2, clValid::connectivity, distance = dist(dat))) %>% 
  set_rownames(NULL)
```

The ranked indices and weights tables are shown below. The PAC uses bounds of 0.05 and 0.95:

```{r worms_summary, results='asis'}
ranked.indices <- index.summary %>% 
  mutate(PAC = rank(PAC, ties.method = "min"),
         DBI = rank(DBI, ties.method = "min"),
         DI = rank(-DI, ties.method = "min"),
         RS = rank(-RS, ties.method = "min"),
         CI = rank(CI, ties.method = "min"),
         GI = rank(-GI, ties.method = "min"),
         CHI = rank(-CHI, ties.method = "min"),
         Conn = rank(Conn, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
pandoc.table(ranked.indices,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.indices) - 1),
                                            collapse = "")))

PAC.CHI.only <- index.summary %>% 
  dplyr::select(Algorithms, PAC, CHI) %>% 
  mutate(Weight.PAC = (1 - PAC) / sum((1 - PAC)),
         Weight.CHI = CHI / sum(CHI),
         Weight = (Weight.PAC + Weight.CHI) / 2) %>% 
  arrange(desc(Weight)) %>% 
  dplyr::select(-Weight.PAC, -Weight.CHI)

pandoc.table(PAC.CHI.only, justify = "lccc")
```

### Meta-Consensus Clusters

Using hierarchical clustering with average linkage on the meta-consensus matrices across the `r ncol(worms.all)` algorithms, we can obtain meta-consensus classes. The following table shows the confusion matrix for the unweighted meta-consensus clusters compared to the source data from `clusterSim`.

```{r worms_meta_unweighted, results='asis'}
meta.cm <- consensusMatrix(worms.all)
meta.cm.confmat <- hclust(dist(meta.cm), method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., clusterSim = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table

cm <- caret::confusionMatrix(meta.cm.confmat)
print(xtable(format(ftable(cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

This next time is the confusion matrix comparing the *weighted* meta-consensus clusters.

```{r worms_meta_weighted, results='asis'}
weights <- PAC.CHI.only[order(match(PAC.CHI.only$Algorithms, colnames(worms.all))), "Weight"]
wt.meta.cm <- consensusMatrix(worms.all, weights)
wt.meta.cm.confmat <- hclust(dist(wt.meta.cm), method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., clusterSim = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table

cm2 <- caret::confusionMatrix(wt.meta.cm.confmat)
print(xtable(format(ftable(cm2$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

The accuracy comparison shows that the meta-consensus clusters have an average performance compared to the individual algorithms.

```{r worms_accuracy, results='asis'}
Accuracy <- alply(worms.all, 2, function(x)
  table(Meta = x, clusterSim = true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  c(list(Meta = meta.cm.confmat,
         Meta.Weighted = wt.meta.cm.confmat)) %>% 
  sapply(., function(x) sum(diag(x)) / sum(x)) %>% 
  data.frame(Accuracy = .) %>% 
  mutate(Algorithm = rownames(.)) %>% 
  dplyr::select(Algorithm, Accuracy) %>% 
  arrange(desc(Accuracy))

pandoc.table(Accuracy, justify = "lc")
```

Finally, let's visualize how the best algorithm, hierarchical clustering using single linkage and Euclidean distance, separates the clusters:

```{r worms_best}
worms.plot <- data.frame(worms.all)
plot(dat, col = c(2:3)[worms.plot$HC..S..Euc.], xlab = "x", ylab = "y",
     main = "K-Means (euclidean) clustering of two atypical parabolic shapes (worms)")
```

It appears that the best algorithm can separate the clusters, yet the indices do not favour such a partition and it *appears* that the algorithm is performing poorly.

## Rings Dataset

The following three-cluster dataset is the second simulation we will conduct:

```{r rings_load}
# Load data, store true cluster labels and number of clusters
set.seed(1)
sc3 <- shapes.circles3()
dat <- sc3$data
true.class <- sc3$clusters
k <- 3

# scatterplot of true clusters
plot(dat, col = c(2:4)[true.class], xlab = "x", ylab = "y",
     main = "Three clusters with atypical ring shapes (circles)")
```

### Indices

The indices for each algorithm are shown below, in no particular order. Also, the ranks are shown.

```{r rings_index, results='asis'}
# combine results
rings.CCP <- readRDS("Rings/outputs/results_CCP.rds")
rings.NMF <- readRDS("Rings/outputs/results_NMF.rds")
rings.all <- rings.CCP %>%
  sapply(., function(x) x[[k]]$consensusClass) %>%
  cbind(., rings.NMF$nmfDiv$consensusClass,
        rings.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES[-which(ALG.NAMES %in% "PAM (MI)")])

# indices
indices <- data.frame(
  Algorithms = ALG.NAMES[-which(ALG.NAMES %in% "PAM (MI)")],
  PAC = rings.CCP %>%
    sapply(., function(x) PAC(x[[k]]$consensusMatrix)) %>%
    c(nmfDiv = PAC(rings.NMF$nmfDiv$consensusMatrix),
      nmfEucl = PAC(rings.NMF$nmfEucl$consensusMatrix)) %>% 
    set_names(ALG.NAMES[-which(ALG.NAMES %in% "PAM (MI)")]),
  DBI = dat %>%
    apply(rings.all, 2, cls.scatt.data, data = .) %>%
    sapply(., clv.Davies.Bouldin,
           intracls = "average", intercls = "average"),
  DI = apply(rings.all, 2, dunn, distance = dist(dat)),
  RS = apply(rings.all, 2, index.S, d = dist(dat)),
  CI = apply(rings.all, 2, index.G3, d = dist(dat)),
  GI = apply(rings.all, 2, index.G2, d = dist(dat)),
  CHI = apply(rings.all, 2, index.G1, x = dat),
  Conn = apply(rings.all, 2, clValid::connectivity, distance = dist(dat))) %>% 
  set_rownames(NULL)

pandoc.table(indices, justify = paste0("l", paste(rep("c", ncol(indices) - 1),
                                                  collapse = "")))
```

And the ranked indices are shown below:

```{r rings_ranks, results='asis'}
ranked.indices <- indices %>% 
  mutate(PAC = rank(PAC, ties.method = "min"),
         DBI = rank(DBI, ties.method = "min"),
         DI = rank(-DI, ties.method = "min"),
         RS = rank(-RS, ties.method = "min"),
         CI = rank(CI, ties.method = "min"),
         GI = rank(-GI, ties.method = "min"),
         CHI = rank(-CHI, ties.method = "min"),
         Conn = rank(Conn, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))

pandoc.table(ranked.indices,
             justify = paste0("l", paste(rep("c", ncol(ranked.indices) - 1),
                                         collapse = "")))
```

### Meta-Consensus Clusters

```{r rings_meta_unweighted}
# weights <- with(ranked.sums, Sum^-1 / sum(Sum^-1))[match(colnames(rings.all), ranked.sums$Algorithms)]
# clust.1 <- apply(rings.all, 1, function(x) which(x == 1))
# clust.2 <- apply(rings.all, 1, function(x) which(x == 2))
# clust.3 <- apply(rings.all, 1, function(x) which(x == 3))
# rings.final <- rings.all %>%
#   as.data.frame %>% 
#   mutate(weight.1 = sapply(clust.1, function(x) sum(weights[x])),
#          weight.2 = sapply(clust.2, function(x) sum(weights[x])),
#          weight.3 = sapply(clust.3, function(x) sum(weights[x])),
#          Meta = ifelse(weight.1 > weight.2 & weight.1 > weight.3, 1,
#                        ifelse(weight.2 > weight.1 & weight.2 > weight.3, 2, 3))) %>% 
#   dplyr::select(-weight.1, -weight.2, -weight.3)
meta.cm <- consensusMatrix(rings.all) %>% 
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., Ref = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>% 
  caret::confusionMatrix(.)
```

The weights are computed using PAC and CHI:

```{r rings_weights, results='asis'}
# algorithms weighted by PAC and CHI
weights.table <- data.frame(Algorithms = ALG.NAMES[-which(ALG.NAMES %in% "PAM (MI)")],
                            PAC = indices$PAC,
                            CHI = indices$CHI) %>%
  mutate(Weight.PAC = (1 - PAC) / sum((1 - PAC)),
         Weight.CHI = CHI / sum(CHI),
         Weight = (Weight.PAC + Weight.CHI) / 2) %>% 
  select(-Weight.PAC, -Weight.CHI) %>% 
  arrange(desc(Weight))
pandoc.table(weights.table, justify = "lccc")
```

```{r rings_meta_weighted, results='asis'}
# confusion matrix of weighted meta.cm with true clusters
wt.meta.cm <- consensusMatrix(rings.all, weights.table$Weight) %>%
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., Ref = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>%
  caret::confusionMatrix(.)
```

Finally, a comparison of the accuracy for each algorithm:

```{r rings_accuracy, results='asis'}
Accuracy <- alply(rings.all, 2, function(x)
  table(Meta = x, Ref = true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  c(list(Meta = meta.cm$table,
         Meta.Weighted = wt.meta.cm$table)) %>% 
  sapply(., function(x) sum(diag(x)) / sum(x)) %>% 
  data.frame(Accuracy = .) %>% 
  mutate(Algorithm = rownames(.)) %>% 
  dplyr::select(Algorithm, Accuracy) %>% 
  arrange(desc(Accuracy))

pandoc.table(Accuracy, justify = "lc")
```

```{r rings_best}
rings.all <- as.data.frame(rings.all)
plot(dat, col = c(2:4)[rings.all$`HC (S. Euc)`], xlab = "x", ylab = "y",
     main = "HC (single linkage euclidean) clustering")
```

Again, the hierarchical clustering using single linkage and euclidean distance perfectly separates the feature space into the three rings.

## Seeds Dataset

```{r seeds_load}
# Load raw data, store true cluster labels and number of clusters
seeds <- readRDS("Seeds/seeds.rds")
true.class <- seeds$Class
k <- 3

# Remove features with low variability and scale
dat <- seeds %>%
  dplyr::select(which(sapply(., class) == "numeric")) %>%
  t %>%
  extract(apply(., 1, sd) > 1, ) %>%
  t %>%
  scale
```

The seeds dataset from [UCI](https://archive.ics.uci.edu/ml/datasets/seeds)
has `r k` classes of seeds with `r unique(table(seeds$Class))` observations each (n = `r nrow(seeds)`). After removing features with low variability, we perform consensus clustering with only area, perimeter, and asymmetry coefficient as the attributes. Each feature is scaled to have mean of 0 and standard deviation of 1.

### Indices

The table below shows the indices for each algorithm in the seeds dataset. Note that the PAC uses bounds of 0 and 1.

```{r seeds_index, results='asis'}
# combine results
seeds.CCP <- readRDS("Seeds/outputs/results_CCP.rds")
seeds.NMF <- readRDS("Seeds/outputs/results_NMF.rds")
seeds.all <- seeds.CCP %>%
  sapply(., function(x) x[[k]]$consensusClass) %>%
  cbind(., seeds.NMF$nmfDiv$consensusClass,
        seeds.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# indices
indices <- data.frame(
  Algorithms = ALG.NAMES,
  PAC = seeds.CCP %>%
    sapply(., function(x) PAC(x[[k]]$consensusMatrix)) %>%
    c(nmfDiv = PAC(seeds.NMF$nmfDiv$consensusMatrix),
      nmfEucl = PAC(seeds.NMF$nmfEucl$consensusMatrix)) %>%
    set_names(ALG.NAMES),
  DBI = dat %>%
    apply(seeds.all, 2, cls.scatt.data, data = .) %>%
    sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average"),
  DI = apply(seeds.all, 2, dunn, distance = dist(dat)),
  RS  = apply(seeds.all, 2, index.S, d = dist(dat)),
  CI = apply(seeds.all, 2, index.G3, d = dist(dat)),
  GI = apply(seeds.all, 2, index.G2, d = dist(dat)),
  CHI = apply(seeds.all, 2, index.G1, x = dat),
  Conn = apply(seeds.all, 2, clValid::connectivity, distance = dist(dat)))

# table of indices
indices %>%
  set_rownames(NULL) %>% 
  pandoc.table(justify = paste0("l", paste(rep("c", ncol(.) - 1),
                                           collapse = "")))
```

And the table of ranked indices is shown below:

```{r seeds_ranks, results='asis'}
# rank the indices
ranked.indices <- indices %>% 
  mutate(PAC = rank(PAC, ties.method = "min"),
         DBI = rank(DBI, ties.method = "min"),
         DI = rank(-DI, ties.method = "min"),
         RS = rank(-RS, ties.method = "min"),
         CI = rank(CI, ties.method = "min"),
         GI = rank(-GI, ties.method = "min"),
         CHI = rank(-CHI, ties.method = "min"),
         Conn = rank(Conn, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))

pandoc.table(ranked.indices,
             justify = paste0("l", paste(rep("c", ncol(ranked.indices) - 1),
                                         collapse = "")))
```

### Meta-Consensus Clusters
The confusion matrix below shows how the unweighted meta-consensus clusters compare to those of UCI.

```{r seeds_meta_unweighted_confmat, results='asis'}
# confusion matrix of meta.cm with true clusters
meta.cm <- consensusMatrix(seeds.all) %>% 
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., UCI = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>% 
  caret::confusionMatrix(.)

# confusion matrix
print(xtable(format(ftable(meta.cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

```{r seeds_meta_unweighted_confstats, results='hide'}
# confusion matrix statistics
pandoc.table(meta.cm$byClass, digits = 3)
```

The weights below are calculated using PAC and CHI. Finally, the weighted meta-consensus clusters confusion matrix with UCI reference label is shown:

```{r seeds_meta_weighted_confmat, results='asis'}
# algorithms weighted by PAC and CHI
weights.table <- data.frame(Algorithms = ALG.NAMES,
                            PAC = indices$PAC,
                            CHI = indices$CHI) %>%
    mutate(Weight.PAC = (1 - PAC) / sum((1 - PAC)),
           Weight.CHI = CHI / sum(CHI),
           Weight = (Weight.PAC + Weight.CHI) / 2)

# confusion matrix of weighted meta.cm with true clusters
wt.meta.cm <- consensusMatrix(seeds.all, weights.table$Weight) %>%
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., UCI = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>%
  caret::confusionMatrix(.)

# weights table
weights.table %>% 
  arrange(desc(Weight)) %>% 
  dplyr::select(-Weight.PAC, -Weight.CHI) %>% 
  pandoc.table(justify = "lccc")

# confusion matrix
print(xtable(format(ftable(wt.meta.cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

```{r seeds_meta_weighted_confstats, results='hide'}
# confusion matrix statistics
pandoc.table(wt.meta.cm$byClass, digits = 3)
```

It appears that the weighted meta-consensus classes have the highest accuracy. This is a comforting result. In real applications, we do not know which individual clustering algorithm would have the best performance. Using the weighted meta-consensus clustering algorithm allows us to avoid making subjective decisions, while also producing the best clustering assignment.

```{r seeds_accuracy, results='asis'}
Accuracy <- alply(seeds.all, 2, function(x)
  table(Meta = x, UCI = true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  c(list(Meta = meta.cm$table,
         Meta.Weighted = wt.meta.cm$table)) %>% 
  sapply(., function(x) sum(diag(x)) / sum(x)) %>% 
  data.frame(Accuracy = .) %>% 
  mutate(Algorithm = rownames(.)) %>% 
  dplyr::select(Algorithm, Accuracy) %>% 
  arrange(desc(Accuracy))

pandoc.table(Accuracy, justify = "lc")
```

More simulations will be performed to confirm the applicability of the meta-consensus clustering method. The datasets should have much more features, likely arising from gene expression data.

## Mice Protein Expression Dataset

```{r mice_load}
# load results and data
mice.CCP <- readRDS("Mice/outputs/results_CCP.rds")
mice.NMF <- readRDS("Mice/outputs/results_NMF.rds")
dat.raw <- read.csv("Mice/Data_Cortex_Nuclear.csv")
dat <- dat.raw %>%
  filter(is_in(class, c("c-CS-m", "c-CS-s", "c-SC-m", "c-SC-s"))) %>%
  select(which(sapply(., class) == "numeric")) %>%
  t %>%
  as.data.frame %>%
  set_colnames(dat.raw$MouseID[dat.raw$class %in%
                                 c("c-CS-m", "c-CS-s", "c-SC-m", "c-SC-s")]) %>%
  extract(, apply(., 2, function(x) !any(is.na(x)))) %>%
  t %>%
  scale

# true number of clusters
k <- 4
```

The mice protein expression dataset from [UCI](http://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) is the most appropriate simulation for the HGSC study. Previously we analyzed data with too few features. The original mice dataset contains expression level data on `r ncol(dat)` proteins measured for `r nrow(dat.raw)` mice. The mice are classified into `r nlevels(dat.raw$class)` groups based on genotype, behaviour and treatment. To reduce computational complexity, we only focus on `r nlevels(dat.raw$class) / 2` groups: *c-CS-s*, *c-CS-m*, *c-SC-s*, and *c-SC-m*. In addition, some samples were removed because they had an `NA` value in at least one protein. No features were removed on the basis of low variability (e.g. SD < 1) across samples in order to maintain high-dimensionality of the data structure. The final dataset used for the simulation has `r nrow(dat)` mice measured on `r ncol(dat)` proteins. 

### Indices

The table of evaluation indices is shown below:

```{r mice_index, results='asis'}
# Combine class labels from CCP and NMF
mice.all <- mice.CCP %>%
  sapply(., function(x) x[[k]]$consensusClass) %>%
  cbind(., mice.NMF$nmfDiv$consensusClass,
        mice.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# mice classes
true.class <- dat.raw$class[match(rownames(mice.all), dat.raw$MouseID)] %>% 
  droplevels

# indices
indices <- data.frame(
  Algorithms = ALG.NAMES,
  PAC = mice.CCP %>%
    sapply(., function(x) PAC(x[[k]]$consensusMatrix)) %>%
    c(nmfDiv = PAC(mice.NMF$nmfDiv$consensusMatrix),
      nmfEucl = PAC(mice.NMF$nmfEucl$consensusMatrix)) %>%
    set_names(ALG.NAMES),
  DBI = dat %>%
    apply(mice.all, 2, cls.scatt.data, data = .) %>%
    sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average"),
  DI = apply(mice.all, 2, dunn, distance = dist(dat)),
  RS  = apply(mice.all, 2, index.S, d = dist(dat)),
  CI = apply(mice.all, 2, index.G3, d = dist(dat)),
  GI = apply(mice.all, 2, index.G2, d = dist(dat)),
  CHI = apply(mice.all, 2, index.G1, x = dat),
  Conn = apply(mice.all, 2, clValid::connectivity, distance = dist(dat)))

# table of indices
indices %>%
  set_rownames(NULL) %>% 
  pandoc.table(justify = paste0("l", paste(rep("c", ncol(.) - 1),
                                           collapse = "")))
```

And also the ranked indices:

```{r mice_ranks, results='asis'}
# rank the indices
ranked.indices <- indices %>% 
  mutate(PAC = rank(PAC, ties.method = "min"),
         DBI = rank(DBI, ties.method = "min"),
         DI = rank(-DI, ties.method = "min"),
         RS = rank(-RS, ties.method = "min"),
         CI = rank(CI, ties.method = "min"),
         GI = rank(-GI, ties.method = "min"),
         CHI = rank(-CHI, ties.method = "min"),
         Conn = rank(Conn, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))

pandoc.table(ranked.indices,
             justify = paste0("l", paste(rep("c", ncol(ranked.indices) - 1),
                                         collapse = "")))
```

### Meta-Consensus Clusters

The confusion matrix below shows how the unweighted meta-consensus clusters compare to those of UCI.

```{r mice_meta_unweighted_confmat, results='asis'}
# confusion matrix of meta.cm with true clusters
meta.cm <- consensusMatrix(mice.all) %>% 
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., UCI = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>% 
  caret::confusionMatrix(.)

# confusion matrix
print(xtable(format(ftable(meta.cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

The weights below are calculated using PAC and CHI. The confusion matrix with the weighted meta-consensus clusters and UCI reference clusters is shown:

```{r mice_meta_weighted_confmat, results='asis'}
# algorithms weighted by PAC and CHI
weights.table <- data.frame(Algorithms = ALG.NAMES,
                            PAC = indices$PAC,
                            CHI = indices$CHI) %>%
  mutate(Weight.PAC = (1 - PAC) / sum((1 - PAC)),
         Weight.CHI = CHI / sum(CHI),
         Weight = (Weight.PAC + Weight.CHI) / 2)

# confusion matrix of weighted meta.cm with true clusters
wt.meta.cm <- consensusMatrix(mice.all, weights.table$Weight) %>%
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., UCI = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>%
  caret::confusionMatrix(.)

# weights table
weights.table %>% 
  arrange(desc(Weight)) %>% 
  dplyr::select(-Weight.PAC, -Weight.CHI) %>% 
  pandoc.table(justify = "lccc")

# confusion matrix
print(xtable(format(ftable(wt.meta.cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

Finally, the accuracy of each algorithm compared to the true class labels:

```{r mice_accuracy, results='asis'}
Accuracy <- alply(mice.all, 2, function(x)
  table(Meta = x, Ref = true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  c(list(Meta = meta.cm$table,
         Meta.Weighted = wt.meta.cm$table)) %>% 
  sapply(., function(x) sum(diag(x)) / sum(x)) %>% 
  data.frame(Accuracy = .) %>% 
  mutate(Algorithm = rownames(.)) %>% 
  dplyr::select(Algorithm, Accuracy) %>% 
  arrange(desc(Accuracy))

pandoc.table(Accuracy, justify = "lc")
```

The weighted meta-consensus clusters gave the highest accuracy, providing further evidence that taking a consensus over algorithms is a feasible notion.

The COMMUNAL method of combining multiple clustering algorithms employs a majority vote on each algoritihm's clustering assignments to arrive at a final set of *core* cluster assignments. Algorithms that assigned at least one cluster a size less than three are dropped from the core cluster computation. The first table below shows the breakdown of the number of samples that remain depending on the level of agreement we require. By default, the percent agreement is 50. The minimum cluster size is 3, and three algorithms did not meet this criteria, so we are left with eight algorithms. The second table shows the breakdown of the core cluster assignments.

```{r mice_COMMUNAL, results='asis'}
mat.key <- clusterKeys(mice.all, k = k)
pandoc.table(examineCounts(mat.key))
core <- returnCore(mat.key)
pandoc.table(table(core))
```

```{r mice_COMMUNAL_accuracy, results='asis'}
COMMUNAL.acc <- table(core, true.class) %>% 
  extract(-1, )
print(xtable(format(ftable(COMMUNAL.acc))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

The accuracy of the COMMUNAL core cluster assignment is `r sum(diag(COMMUNAL.acc)) / sum(table(core))`.

## Khan Mircoarray Data

```{r khan_load}
khan.CCP <- readRDS("Khan/outputs/results_CCP.rds")
khan.NMF <- readRDS("Khan/outputs/results_NMF.rds")

# Randomly choose 16 EWS and 16 RMS samples
set.seed(1)
n.samp <- 16
data(khan)
dat <- khan %>% 
  set_rownames(.$X) %>% 
  dplyr::select(-X, -X1) %>% 
  set_colnames(paste0(names(.), "_", as.character(unlist(.[1, ])))) %>% 
  extract(-1, ) %>% 
  apply(., 2, function(x) as.numeric(as.character(x))) %>% 
  as.data.frame %>% 
  extract(, c(sample(grep("EWS", names(.)), n.samp),
              sample(grep("RMS", names(.)), n.samp))) %>% 
  extract(apply(., 1, sd) > 1, ) %>%
  t %>%
  scale

# true number of clusters
k <- 2
```

### Indices

```{r khan_index, results='asis', warning=FALSE}
# Combine class labels from CCP and NMF
khan.all <- khan.CCP %>%
  sapply(., function(x) x[[k]]$consensusClass) %>%
  cbind(., khan.NMF$nmfDiv$consensusClass,
        khan.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# khan classes
true.class <- str_sub(rownames(dat), start = -3)

# indices
indices <- data.frame(
  Algorithms = ALG.NAMES,
  PAC = khan.CCP %>%
    sapply(., function(x) PAC(x[[k]]$consensusMatrix, 0.05, 0.95)) %>%
    c(nmfDiv = PAC(khan.NMF$nmfDiv$consensusMatrix, 0.05, 0.95),
      nmfEucl = PAC(khan.NMF$nmfEucl$consensusMatrix, 0.05, 0.95)) %>%
    set_names(ALG.NAMES),
  DBI = dat %>%
    apply(khan.all, 2, cls.scatt.data, data = .) %>%
    sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average"),
  DI = apply(khan.all, 2, dunn, distance = dist(dat)),
  RS  = apply(khan.all, 2, index.S, d = dist(dat)),
  CI = apply(khan.all, 2, index.G3, d = dist(dat)),
  GI = apply(khan.all, 2, index.G2, d = dist(dat)),
  CHI = apply(khan.all, 2, index.G1, x = dat),
  Conn = apply(khan.all, 2, clValid::connectivity, distance = dist(dat)))

# table of indices
indices %>%
  set_rownames(NULL) %>% 
  pandoc.table(justify = paste0("l", paste(rep("c", ncol(.) - 1),
                                           collapse = "")))
```

### Meta-Consensus Clusters

```{r khan_meta_unweighted_confmat, results='asis'}
# confusion matrix of meta.cm with true clusters
meta.cm <- consensusMatrix(khan.all) %>% 
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., pamr = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>% 
  caret::confusionMatrix(.)

# confusion matrix
print(xtable(format(ftable(meta.cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

```{r khan_meta_weighted_confmat, results='asis'}
# algorithms weighted by PAC and CHI
weights.table <- data.frame(Algorithms = ALG.NAMES,
                            PAC = indices$PAC,
                            CHI = indices$CHI) %>%
  mutate(Weight.PAC = (1 - PAC) / sum((1 - PAC)),
         Weight.CHI = CHI / sum(CHI),
         Weight = (Weight.PAC + Weight.CHI) / 2)

# confusion matrix of weighted meta.cm with true clusters
wt.meta.cm <- consensusMatrix(khan.all, weights.table$Weight) %>%
  dist %>% 
  hclust(method = "average") %>%
  cutree(k) %>%
  as.factor %>%
  set_names(true.class) %>%
  table(Meta = ., UCI = names(.)) %>%
  extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table %>%
  caret::confusionMatrix(.)

# weights table
weights.table %>% 
  arrange(desc(Weight)) %>% 
  dplyr::select(-Weight.PAC, -Weight.CHI) %>% 
  pandoc.table(justify = "lccc")

# confusion matrix
print(xtable(format(ftable(wt.meta.cm$table))), comment = F,
      include.rownames = F, include.colnames = F,
      sanitize.text.function = function(x) gsub('"', "", x))
```

```{r khan_accuracy, results='asis'}
Accuracy <- alply(khan.all, 2, function(x)
  table(Meta = x, Ref = true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  c(list(Meta = meta.cm$table,
         Meta.Weighted = wt.meta.cm$table)) %>% 
  sapply(., function(x) sum(diag(x)) / sum(x)) %>% 
  data.frame(Accuracy = .) %>% 
  mutate(Algorithm = rownames(.)) %>% 
  dplyr::select(Algorithm, Accuracy) %>% 
  arrange(desc(Accuracy))

pandoc.table(Accuracy, justify = "lc")
```

# References

1. Brunet, J. P., Tamayo, P., Golub, T. R., & Mesirov, J. P. (2004). "Metagenes and molecular pattern discovery using matrix factorization." *Proceedings of the national academy of sciences*, 101 (12), 4164-4169.

2. Monti, S., Tamayo, P., Mesirov, J., & Golub, T. (2003). "Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data." *Machine learning*, 52 (1-2), 91-118.

3. Cancer Genome Atlas Research Network. (2011). "Integrated genomic analyses of ovarian carcinoma." *Nature*, 474 (7353), 609-615.

4. Conrad, J. G., Al-Kofahi, K., Zhao, Y., & Karypis, G. (2005, June). "Effective document clustering for large heterogeneous law firm collections." In *Proceedings of the 10th international conference on Artificial intelligence and law* (pp. 177-187). ACM.

5. Cohen, J. (1960). "A coefficient of agreement for nominal scales." *Educational and psychological measurement*, 20 (1), 37-46.

6. Cohen, J. (1968). "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit." *Psychological bulletin*, 70 (4), 213.

7. Fleiss, J. L. (1981). *Statistical methods for rates and proportions* (2nd ed.). New York: John Wiley.

8. Rand, W. M. (1971). "Objective criteria for the evaluation of clustering methods." *Journal of the American Statistical Association*, 66 (336), 846-850.

9. Hubert, L., & Arabie, P. (1985). "Comparing partitions." *Journal of Classification*, 2 (1), 193-218.

10. Vinh, N. X., Epps, J., & Bailey, J. (2009, June). "Information theoretic measures for clusterings comparison: is a correction for chance necessary?." *In Proceedings of the 26th Annual International Conference on Machine Learning* (pp. 1073-1080). ACM.

11. Senbabaoglu, Y., Michailidis, G., & Li, J. Z. (2014). "Critical limitations of consensus clustering in class discovery." *Scientific reports*, 4.

12. Davies, D. L., & Bouldin, D. W. (1979). "A Cluster Separation Measure." *IEEE Transactions on Pattern Analysis and Machine Intelligence*. PAMI-1 (2): 224–227.

13. Dunn, J. C. (1973). "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters." *Journal of Cybernetics* 3 (3): 32–57.

14. Rousseeuw, P. J. (1987). "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis." *Journal of Computational and Applied Mathematics*, 20, 53-65.

15. Hubert, L. J., & Levin, J. R. (1976). "A general statistical framework for assessing categorical clustering in free recall." *Psychological bulletin*, 83 (6), 1072-1080.

16. Baker, F. B., & Hubert, L. J. (1975). "Measuring the power of hierarchical cluster analysis." *Journal of the American Statistical Association*, 70 (349), 31-38.

17. Calinski, T., & Harabasz, J. (1974). "A dendrite method for cluster analysis." *Communications in Statistics-theory and Methods*, 3 (1), 1-27.

18. Handl, J., Knowles, J., & Kell, D. B. (2005). "Computational cluster validation in post-genomic data analysis." *Bioinformatics*, 21 (15), 3201-3212.
