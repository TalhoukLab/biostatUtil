---
title: "Performance Evaluation of Consensus Clustering Algorithms"
subtitle: "STAT 598 Progress Report"
author: "Derek Chiu"
date: "August 25, 2015"
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 3
---

# Preface
This progress report fulfills the UBC Science Co-op requirement to submit a work term report at the end of every four month period. BC Cancer Agency (BCCA) is a not-for-profit organization that aims to provide care for cancer patients and conduct innovative cancer research. Our department, OvCaRe, is the Ovarian Cancer Research team tasked with studying ovarian cancers of many types. The objective of the project I am working on is to discover a viable classifer for ovarian high-grade serous carcinoma (HGSC). My role is to help devise a clustering algorithm that can statistically partition subtypes of HGSC without knowledge of the pathological properties of each sample. The progress report will evaluate the method we are using, consensus clustering, on a publicly available data set as well as simulated data sets. The final technical report will contain results of our method applied on HGSC data from our own cohort.

```{r setup, echo=FALSE, cache=FALSE, include=TRUE, message=FALSE}
# packages
library(tcgaHGSC)
library(ConClust)
library(clv)
library(clusterSim)
library(NMF)
library(mclust)
library(clue)
library(ggplot2)
library(RColorBrewer)
library(pander)
library(tidyr)
library(plyr)
library(dplyr)
library(magrittr)
library(knitr)
panderOptions("table.split.table", 300)  # don't split any tables
opts_chunk$set(echo = FALSE)  # hide all code from output
# names of clustering algorithms
ALG.NAMES <- c("HC (Euclidean)", "HC (Diana)",
               "KM (Euclidean)", "KM (Spearman)", "KM (MI)",
               "PAM (Euclidean)", "PAM (Spearman)", "PAM (MI)",
               "NMF (Divergence)", "NMF (Euclidean)")
```

```{r load}
results.CCP <- readRDS("TCGA/outputs/results_CCP.rds")
results.NMF <- readRDS("TCGA/outputs/results_NMF.rds")
data(hgsc)
dat <- hgsc %>%
  set_rownames(.$UNIQID) %>%
  select(which(sapply(., class) == "numeric")) %>%
  magrittr::extract(apply(., 1, sd) > 1, ) %>%
  t %>%
  scale
```

# Introduction
Unsupervised learning is the process of inferring something about a data structure without knowing its true class labels. Cluster analysis is an unsupervised learning method of assigning entities into different groups based on one or more of their attributes. It is unsupervised because we do not know the true partitions of the entities. The objective is to place similar objects together in the same cluster and separate dissimilar objects into different clusters. For example, in genomics studies, we frequently try and cluster patient samples measured on a large number of molecular features. When we get a clustering assignment from an algorithm, we often want to evaluate its performance. Ideally, a good clustering algorithm is able to differentiate entities without knowledge of the true class labels. In addition, we want the algorithm to arrive at a stable and optimal number of clusters. The choice of the number of clusters is not trivial in some cases.

# Methods

## Clustering Algorithms
There are many clustering algorithms, each approaching the clustering problem in a different way. It is most important to note the advantages and limitations of each algorithm. There are some definitions to take note of:

- **Compactness**: how close the objects are in each cluster
- **Connectivity**: how connected the objects are in the feature space

### Hierarchical Clustering
This clustering algorithm is very popular because of its intuitive representation using dendrograms (trees). Based on a distance matrix, the objects/features are clustered based on a linkage type. More similar objects are joined near the bottom of a dendrogram whereas less similar objects are joined at a higher tree height. A linkage criterion determines the distances amongst a set of objects/features using the pairwise distances. For example, an average linkage would use the average pairwise distances. In this way, a dendrogram with all objects/features can be made by recursively linking larger and larger sets of observations together. 

CITATION: it turns out that single linkage works very well for data sets exhibiting connectivitiy but not compactness. An example of this would be a tree rings. Clusters are circles, and objects that are far away are in the same cluster compared to objects that are close. On the other hand, average linkage works well for data sets exhibiting compactness.

### k-Means Clustering
First, k means are randomly initialized in the multi-dimensional object/feature space that we wish to cluster. Assigning each object/feature to its closest mean forms the clusters. The k means are re-calculated based on the centroids (center points) within each cluster. This process is repeated until the centroids converge.

There are two caveats to note when using k-means. First, the cluster assignments are unstable because they depend on the random initialization of the means. We would preferably want to repeat this process many times to see whether the clusters change drastically. Secondly, the choice of k is not arbitrary. Cross-validation using an appropriate loss function is a popular method for choosing k.

### k-Medoids Clustering
Very similar to k-means except that we initialize a set of random data points as medoids instead of random means that are generally not real data points. The most common version of k-medoids is PAM.

### Nonnegative Matrix Factorization
Given a non-negative data matrix A, we can factor it into two matrices W and H, which are also non negative. W and H have important properties. Suppose A has genes as rows and samples as columns. If we are interested in clustering samples, then H has a reduced gene space of meta genes that fully explain the samples. Samples are clustered based on the metagene they are most associated with. If we are interested in clustering genes, then W has a reduced sample space of metasamples that fully explain the genes. Genes are clustered based on the metasample they are most associated with. 

In gene expression data, it is common to standardize the genes. Doing so would disrupt the nonnegativity of A required for NMF. A simple remedy can solve this problem. We append the matrix â€“A to the bottom of A, preserving the same number of columns, and set all the negative entries to 0. The computational complexity has been doubled as a result. NMF takes a long time to run, but offers the most stable clustering assignments.

## Consensus Clustering
Consensus clustering is as much an algorithm itself as it is a way of combining realizations of other clustering algorithms. For this, it deserves its own section. Algorithms like k-means and PAM are unstable, as the clustering assignments depend on the initialization of centroids and medoids, respectively. Consensus clustering compares results from repeated runs of a clustering algorithm. Typically, these runs use different subsamples of the data to model sampling variability. A final clustering assignment is determined based on agreement across replicates.

A consensus matrix is a significant aspect of consensus clustering. All entries range from 0 to 1, and represent the proportion of replicates in which two objects/features were clustered together. A perfect clustering would consist of a consensus matrix with only 0s and 1s. More importantly, we can perform hierarchical clustering on a consensus matrix, resulting in a heat map with a diagonal block structure.

Repeating this process for different values of k (number of clusters) and looking at the corresponding heatmaps, we can see which value of k provides the most stable clustering assignment. Certainly, using a performance measure such as PAC would be a more formal method of assessment that doesn't rely on potentially subjective visualizations.

Consensus clustering attempts to stabilize results from randomness introduced by subsampling and the clustering algorithm. A natural extension is sometimes called meta consensus clustering, where we want to compare results across different algorithms. For instance, we may combine the consensus clustering assignments from 10 different algorithms and come up with a final clustering.

```{r all_clusters}
results.all <- results.CCP %>%
  sapply(., function(x) x[[4]]$consensusClass) %>%
  cbind(., results.NMF$nmfDiv$consensusClass,
        results.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

meta.cm <- consensusMatrix(results.all)
BuPuFun <- colorRampPalette(brewer.pal(n = 9, "BuPu"))
palBuPu <- BuPuFun(256)
heatmap(meta.cm, labRow = NA, labCol = NA, col = palBuPu)
```

The proportion of cases with at least 0.6 agreement is `r ((sum(meta.cm >= 0.6) - ncol(meta.cm)) / 2) / choose(ncol(meta.cm), 2)`.

```{r conf_mat}
final.compare <- hclust(dist(meta.cm), method = "average") %>%
  cutree(4) %>%
  as.factor %>%
  set_names(hclust(dist(meta.cm), method = "average")$labels) %>%
  set_names(substring(names(.), first = 18)) %>%
  table(., names(.)) %>%
  magrittr::extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table

names(dimnames(final.compare)) <- c("Predicted", "Reference")
cm <- caret::confusionMatrix(final.compare)
```

The confusion matrix is shown below, as well as different metrics for each class.

```{r conf_mat_output, results='asis'}
pandoc.table(cm$table)
pandoc.table(cm$byClass)
```

# External Evaluation
External evaluation usually refers to the case when we compare our clustering assignments to true class labels, or have some gold standard to compare to. In applications, this might be the published clustering result. The downside of using external evaluation is that the reference classes may not be correctly clustered themselves, and we are treating these as the norm. None the less, we can explore a few metrics.

## Purity and Entropy

Purity is defined as the sum of the entities of maximal class in each cluster divided by the total number of entities. The equation is:
\begin{center}
$Purity = \dfrac{1}{n}\sum\limits_{r = 1}^{k}\max\limits_{i}(n_r^i)$
\end{center}
where $n$ is the total number of entities, $k$ is the number of clusters, $i$ is a particular class, and $n_r^i$ is the number of objects classified into class $i$ in cluster $r$. The larger the purity, the better the clustering accuracy.
```{r purity}
true.class <- as.factor(substring(rownames(results.all), first = 18))
Purity <- apply(results.all, 2, function(x) purity(as.factor(x), true.class))
sort(Purity, decreasing = T)
```

Entropy measures the amount of uncertainty in each cluster. The equation is:
\begin{center}
$Entropy = -\dfrac{1}{n\log{q}}\sum\limits_{r = 1}^{k}\sum\limits_{i = 1}^{q}n_r^i\log{\dfrac{n_r^i}{n_r}}$
\end{center}
The smaller the entropy, the less uncertain we are of the cluster membership, and the better the clustering performance.
```{r entropy}
Entropy <- apply(results.all, 2, function(x) entropy(as.factor(x), true.class))
sort(Entropy)
```

## Kappa Statistics

```{r kappa}
kappa <- psych::wkappa(final.compare)
```

The unadjusted kappa statistic is `r kappa$kappa` and the weighted kappa statistic is `r kappa$weighted.kappa` for the final meta consensus cluster.
 
## Adjusted Rand Index

The larger the better.

```{r ARI, results='asis'}
ARI <- apply(results.all, 2, adjustedRandIndex, x = true.class) %>% 
  sort(decreasing = T) %>% 
  data.frame(ARI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, ARI)
pandoc.table(ARI, justify = "lc")
```

# Internal Evaluation

## Proportion of Ambiguously Clustered Pairs (PAC)

```{r CDF}
CDF.each <- list(nmfDiv = results.NMF$nmfDiv$consensusMatrix,
                 nmfEucl = results.NMF$nmfEucl$consensusMatrix) %>%
  ldply(.fun = function(x) x[lower.tri(x)]) %>%
  rbind(., ldply(results.CCP, function(x)
    x[[4]]$consensusMatrix[lower.tri(x[[4]]$consensusMatrix)])) %>%
  as.data.frame %>%
  set_rownames(.$.id) %>%
  select(-.id) %>%
  t %>%
  as.data.frame %>%
  gather(key = Method, value = CDF, 1:ncol(.))

ggplot(CDF.each, aes(x = CDF, colour = Method)) +
  stat_ecdf() +
  facet_wrap(~ Method) +
  theme(legend.position = "none")
```

The lower the better.

```{r PAC, results='asis'}
PAC <- results.CCP %>%
  sapply(., function(x) PAC(x[[4]]$consensusMatrix)) %>%
  c(nmfDiv = PAC(results.NMF$nmfDiv$consensusMatrix),
    nmfEucl = PAC(results.NMF$nmfEucl$consensusMatrix)) %>%
  set_names(ALG.NAMES) %>% 
  sort %>% 
  data.frame(PAC = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, PAC)
pandoc.table(PAC, justify = "lc")
```

The PAC for the meta consensus matrix is `r PAC(meta.cm)`.

## Davies-Bouldin Index

For DBI, the lower the better.

```{r DBI, results='asis'}
DBI <- dat %>%
  apply(results.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average") %>%
  sort %>% 
  data.frame(DBI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, DBI)
pandoc.table(DBI, justify = "lc")
```

## Dunn Index

For DI, the larger the better.

```{r DI, results='asis'}
DI <- dat %>%
  apply(results.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Dunn, intracls = "average", intercls = "average") %>%
  sort(decreasing = T) %>% 
  data.frame(DI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, DI)
pandoc.table(DI, justify = "lc")
```

## Rousseeuw's Silhouette

For Rousseuw's Silhouette internal cluster quality index (RS), the larger the better.

```{r RS, results='asis'}
RS <- apply(results.all, 2, index.S, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(RS = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, RS)
pandoc.table(RS, justify = "lc")
```

## Silhouette Average Width

For SAW, the larger the better.

```{r SAW, results='asis'}
SAW <- apply(results.all, 2, function(x) summary(silhouette(x, dist(dat)))$avg.width) %>%
  sort(decreasing = T) %>% 
  data.frame(SAW = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, SAW)
pandoc.table(SAW, justify = "lc")
```

## C-Index

For CI, the lower the better.

```{r CI, results='asis'}
CI <- apply(results.all, 2, index.G3, d = dist(dat)) %>%
  sort %>% 
  data.frame(CI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, CI)
pandoc.table(CI, justify = "lc")
```

## Gamma Index

For BHI, the larger the better.

```{r BHI, results='asis'}
BHI <- apply(results.all, 2, index.G2, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(BHI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, BHI)
pandoc.table(BHI, justify = "lc")
```

## CH Index

For CHI, the larger the better.

```{r CHI, results='asis'}
CHI <- apply(results.all, 2, index.G1, x = dat) %>%
  sort(decreasing = T) %>% 
  data.frame(CHI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, CHI)
pandoc.table(CHI, justify = "lc")
```

## Summary

Here is a summary of all the indices for each algorithm, in unsorted order.

```{r index_summary, results='asis'}
index.summary <- Reduce(merge, list(PAC, DBI, DI, SAW, RS, CI, BHI, CHI))
pandoc.table(index.summary,
             justify = paste0("l",
                              paste(rep("c", ncol(index.summary) - 1),
                                    collapse = "")))
```

# Ranked Indices

The table below shows the ranking of algorithms for performance on a clustering index, for each index. There is an additional column that shows the propoportion of indices where an algorithm was ranked **first or second**.

```{r ranked_indices, results='asis'}
ranked.indices <- data.frame(Algorithms = index.summary$Algorithms,
                             PAC = rank(index.summary$PAC),
                             DBI = rank(index.summary$DBI),
                             DI = rank(-index.summary$DI),
                             SAW = rank(-index.summary$SAW),
                             RS = rank(-index.summary$RS),
                             CI = rank(index.summary$CI),
                             BHI = rank(-index.summary$BHI),
                             CHI = rank(-index.summary$CHI)) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
  
pandoc.table(ranked.indices,
             justify = paste0("l",
                              paste(rep("c", ncol(ranked.indices) - 1),
                                    collapse = "")))
```

If we were to conduct a meta consensus clustering, a weight for each algorithm needs to be assigned. One such way of doing so is using the inverse rank sums. We can sum the ranks for each algorithm, and assign higher weight for consistently high ranked methods (1st or 2nd) and vice versa. This is shown below:

```{r inverse_rank_sum, results='asis'}
ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 10)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)

pandoc.table(ranked.sums,
             justify = paste0("l",
                              paste(rep("c", ncol(ranked.sums) - 1),
                                    collapse = "")))
```

# Simulations

To confirm the clustering results from using the TCGA dataset, we need to try out the algorithms on a few simulated datasets, designed to test the robustness of each method. The `clusterSim` package provides very good built-in examples to use.

## Worms Dataset

The following two-cluster dataset is our first simulation:

```{r worms_plot}
set.seed(1)
sw <- shapes.worms()
dat <- sw$data
true.class <- sw$clusters
plot(dat, col = c(2:3)[true.class], xlab = "x", ylab = "y",
     main = "Two clusters with atypical parabolic shapes (worms)")
```

```{r worms_index}
# combine results
worms.CCP <- readRDS("Worms/outputs/results_CCP.rds")
worms.NMF <- readRDS("Worms/outputs/results_NMF.rds")
worms.all <- worms.CCP %>%
  sapply(., function(x) x[[2]]$consensusClass) %>%
  cbind(., worms.NMF$nmfDiv$consensusClass,
        worms.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# indices
PAC <- worms.CCP %>%
  sapply(., function(x) PAC(x[[2]]$consensusMatrix)) %>%
  c(nmfDiv = PAC(worms.NMF$nmfDiv$consensusMatrix),
    nmfEucl = PAC(worms.NMF$nmfEucl$consensusMatrix)) %>% 
  set_names(ALG.NAMES)
DBI <- dat %>%
  apply(worms.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average")
DI <- dat %>%
  apply(worms.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Dunn, intracls = "average", intercls = "average")
SAW <- apply(worms.all, 2, function(x) summary(silhouette(x, dist(dat)))$avg.width)
RS <- apply(worms.all, 2, index.S, d = dist(dat))
CI <- apply(worms.all, 2, index.G3, d = dist(dat))
BHI <- apply(worms.all, 2, index.G2, d = dist(dat))
CHI <- apply(worms.all, 2, index.G1, x = dat)
```

The ranked indices and weights tables are shown below:

```{r worms_summary, results='asis'}
ranked.indices <- data.frame(Algorithms = ALG.NAMES,
                             PAC = rank(PAC, ties.method = "min"),
                             DBI = rank(DBI, ties.method = "min"),
                             DI = rank(-DI, ties.method = "min"),
                             SAW = rank(-SAW, ties.method = "min"),
                             RS = rank(-RS, ties.method = "min"),
                             CI = rank(CI, ties.method = "min"),
                             BHI = rank(-BHI, ties.method = "min"),
                             CHI = rank(-CHI, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
pandoc.table(ranked.indices,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.indices) - 1),
                                            collapse = "")))

ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 10)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  dplyr::select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)
pandoc.table(ranked.sums,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.sums) - 1),
                                            collapse = "")))
```

Using hierarchical clustering with Wald's method on the meta-consensus matrix across the ten algorithms, we can obtain meta-consensus classes. The following table shows how the different methods compare, based on the number of matches to the true class labels.

```{r worms_meta, results='asis'}
weights <- with(ranked.sums, Sum^-1 / sum(Sum^-1))[match(colnames(worms.all), ranked.sums$Algorithms)]
clust.1 <- apply(worms.all, 1, function(x) which(x == 1))
clust.2 <- apply(worms.all, 1, function(x) which(x == 2))
worms.final <- worms.all %>%
  as.data.frame %>% 
  mutate(weight.1 = sapply(clust.1, function(x) sum(weights[x])),
         weight.2 = sapply(clust.2, function(x) sum(weights[x])),
         Meta = ifelse(weight.1 > weight.2, 1, 2)) %>% 
  select(-weight.1, -weight.2)

meta.classes <- as.factor(cutree(hclust(dist(consensusMatrix(worms.all)), method = "ward.D"), 2))

worms.matches <- worms.final %>% 
  apply(., 2, function(x) sum(x == true.class)) %>% 
  sort(decreasing = T) %>% 
  data.frame(Matches = .)

pandoc.table(worms.matches)
```

Let's visualize how the best algorithm, K-Means using euclidean distance, separates the clusters:

```{r worms_best}
plot(sw$data, col = c(2:3)[worms.final$`KM (Euclidean)`], xlab = "x", ylab = "y",
     main = "K-Means (euclidean) clustering of two atypical parabolic shapes (worms)")
```

It appears that even the best algorithm cannot make a parabolic division in the feature space.

## Rings Dataset

The following three-cluster dataset is our second simulation:

```{r rings_plot}
set.seed(1)
sc3 <- shapes.circles3()
dat <- sc3$data
true.class <- sc3$clusters
plot(dat, col = c(2:4)[true.class], xlab = "x", ylab = "y",
     main = "Three clusters with atypical ring shapes (circles)")
```

```{r rings_index}
# combine results
rings.CCP <- readRDS("Rings/outputs/results_CCP.rds")
rings.NMF <- readRDS("Rings/outputs/results_NMF.rds")
rings.all <- rings.CCP %>%
  sapply(., function(x) x[[2]]$consensusClass) %>%
  cbind(., rings.NMF$nmfDiv$consensusClass,
        rings.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES[-8])

# indices
PAC <- rings.CCP %>%
  sapply(., function(x) PAC(x[[2]]$consensusMatrix)) %>%
  c(nmfDiv = PAC(rings.NMF$nmfDiv$consensusMatrix),
    nmfEucl = PAC(rings.NMF$nmfEucl$consensusMatrix)) %>% 
  set_names(ALG.NAMES[-8])
DBI <- dat %>%
  apply(rings.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average")
DI <- dat %>%
  apply(rings.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Dunn, intracls = "average", intercls = "average")
SAW <- apply(rings.all, 2, function(x) summary(silhouette(x, dist(dat)))$avg.width)
RS <- apply(rings.all, 2, index.S, d = dist(dat))
CI <- apply(rings.all, 2, index.G3, d = dist(dat))
BHI <- apply(rings.all, 2, index.G2, d = dist(dat))
CHI <- apply(rings.all, 2, index.G1, x = dat)
```

```{r rings_summary, results='asis'}
ranked.indices <- data.frame(Algorithms = ALG.NAMES[-8],
                             PAC = rank(PAC, ties.method = "min"),
                             DBI = rank(DBI, ties.method = "min"),
                             DI = rank(-DI, ties.method = "min"),
                             SAW = rank(-SAW, ties.method = "min"),
                             RS = rank(-RS, ties.method = "min"),
                             CI = rank(CI, ties.method = "min"),
                             BHI = rank(-BHI, ties.method = "min"),
                             CHI = rank(-CHI, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
pandoc.table(ranked.indices,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.indices) - 1),
                                            collapse = "")))

ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 10)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  dplyr::select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)
pandoc.table(ranked.sums,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.sums) - 1),
                                            collapse = "")))
```

```{r rings_meta, results='asis'}
# weights <- with(ranked.sums, Sum^-1 / sum(Sum^-1))[match(colnames(rings.all), ranked.sums$Algorithms)]
# clust.1 <- apply(rings.all, 1, function(x) which(x == 1))
# clust.2 <- apply(rings.all, 1, function(x) which(x == 2))
# clust.3 <- apply(rings.all, 1, function(x) which(x == 3))
# rings.final <- rings.all %>%
#   as.data.frame %>% 
#   mutate(weight.1 = sapply(clust.1, function(x) sum(weights[x])),
#          weight.2 = sapply(clust.2, function(x) sum(weights[x])),
#          weight.3 = sapply(clust.3, function(x) sum(weights[x])),
#          Meta = ifelse(weight.1 > weight.2 & weight.1 > weight.3, 1,
#                        ifelse(weight.2 > weight.1 & weight.2 > weight.3, 2, 3))) %>% 
#   select(-weight.1, -weight.2, -weight.3)

# meta.classes <- as.factor(cutree(hclust(dist(consensusMatrix(rings.all)), method = "ward.D"), 2))
rings.final <- rings.all %>% 
  as.data.frame %>% 
  mutate(Meta = as.factor(cutree(hclust(dist(consensusMatrix(rings.all)), method = "ward.D"), 3)))

rings.matches <- rings.final %>% 
  apply(., 2, function(x) sum(x == true.class)) %>% 
  sort(decreasing = T) %>% 
  data.frame(Matches = .)

pandoc.table(rings.matches)
```

```{r rings_best}
plot(dat, col = c(2:4)[rings.final$`PAM (Euclidean)`], xlab = "x", ylab = "y",
     main = "PAM (euclidean) clustering of three circles (rings)")
```

Again, we are only able to make linear separations.

# References

Monti, S., Tamayo, P., Mesirov, J., & Golub, T. (2003). "Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data." *Machine learning*, 52 (1-2), 91-118.

Conrad, J. G., Al-Kofahi, K., Zhao, Y., & Karypis, G. (2005, June). "Effective document clustering for large heterogeneous law firm collections." In *Proceedings of the 10th international conference on Artificial intelligence and law* (pp. 177-187). ACM.

Rand, W. M. (1971). "Objective criteria for the evaluation of clustering methods." *Journal of the American Statistical Association*, 66 (336), 846-850.

Hubert, L., & Arabie, P. (1985). "Comparing partitions." *Journal of Classification*, 2 (1), 193-218.

Vinh, N. X., Epps, J., & Bailey, J. (2009, June). "Information theoretic measures for clusterings comparison: is a correction for chance necessary?." *In Proceedings of the 26th Annual International Conference on Machine Learning* (pp. 1073-1080). ACM.

Senbabaoglu, Y., Michailidis, G., & Li, J. Z. (2014). "Critical limitations of consensus clustering in class discovery." *Scientific reports*, 4.

Davies, David L.; Bouldin, Donald W. (1979). "A Cluster Separation Measure." *IEEE Transactions on Pattern Analysis and Machine Intelligence*. PAMI-1 (2): 224â€“227.

Dunn, J. C. (1973). "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters." *Journal of Cybernetics* 3 (3): 32â€“57.

Rousseeuw, P. J. (1987). "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis." *Journal of Computational and Applied Mathematics*, 20, 53-65.

Hubert, L. J., & Levin, J. R. (1976). "A general statistical framework for assessing categorical clustering in free recall." *Psychological bulletin*, 83 (6), 1072-1080.

Baker, F. B., & Hubert, L. J. (1975). "Measuring the power of hierarchical cluster analysis." *Journal of the American Statistical Association*, 70 (349), 31-38.

Calinski, T., & Harabasz, J. (1974). "A dendrite method for cluster analysis." *Communications in Statistics-theory and Methods*, 3 (1), 1-27.

