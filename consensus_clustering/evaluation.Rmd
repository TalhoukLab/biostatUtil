---
title: "Performance Evaluation of Consensus Clustering Algorithms"
subtitle: "STAT 598 Progress Report"
author: "Derek Chiu"
date: "September 1, 2015"
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 3
---

# Preface
This progress report fulfills the UBC Science Co-op requirement to submit a work term report at the end of every four month period. BC Cancer Agency (BCCA) is a not-for-profit organization that aims to provide care for cancer patients and conduct innovative cancer research. Our department, OvCaRe, is the Ovarian Cancer Research team tasked with studying ovarian cancers of many types. The objective of the project I am working on is to discover a viable classifier for ovarian high-grade serous carcinoma (HGSC). My role is to help devise a clustering algorithm that can partition tumour samples of HGSC into different subtypes without knowledge of the underlying pathological properties of each sample. Instead, only gene expression data will be used in the prediction. The progress report will evaluate the method we are using, consensus clustering, on a publicly available data set as well as some simulated data sets. The final technical report will contain results of our method applied on HGSC data from our own cohort.

```{r setup, echo=FALSE, cache=FALSE, include=TRUE, message=FALSE}
# packages
library(tcgaHGSC)
library(ConClust)
library(clv)
library(clValid)
library(clusterSim)
library(NMF)
library(mclust)
library(clue)
library(gplots)
library(ggplot2)
library(RColorBrewer)
library(pander)
library(plyr)
library(dplyr)
library(magrittr)
library(knitr)
panderOptions("table.split.table", 300)  # don't split any tables
opts_chunk$set(echo = FALSE)  # hide all code from output
# names of clustering algorithms
ALG.NAMES <- c("HC (Avg Euc)", "HC (Sing Euc)", "HC (Diana)",
               "KM (Euc)", "KM (Spear)", "KM (MI)",
               "PAM (Euc)", "PAM (Spear)", "PAM (MI)",
               "NMF (Div)", "NMF (Euc)")
```

# Introduction
Unsupervised learning is the process of inferring something about a data structure without knowing its true class labels. Cluster analysis is an unsupervised learning method of assigning entities into different groups based on one or more of their attributes. It is unsupervised because we do not know the true partitions of the entities. The goal is to place similar objects together in the same cluster and separate dissimilar objects into different clusters. For example, in genomics studies, we frequently try and cluster patient samples measured on a large number of molecular features.

When we obtain a clustering assignment from an algorithm, we often want to evaluate its performance and validity. Ideally, a good clustering algorithm is able to differentiate entities without knowledge of the true class labels. In addition, we want the algorithm to arrive at a stable clustering result. Some algorithms are sensitive to initial conditions and we do not want the assignments to be dependent on those. Finally, the choice of the number of clusters is not trivial in unsupervised explorations. This will not be a problem in simulations because we do know the true class labels. However, it is important to keep in mind that for real data the number of clusters should be determined using the data structure.

# Methods

## Clustering Algorithms
There are many clustering algorithms, each approaching the problem in a different way. It is important to note the advantages and limitations of each algorithm. These are some definitions of clustering performance^18^:

- **Compactness**: how close together or tight objects are to each other within a cluster
- **Separation**: how far apart objects are in different clusters
- **Connectivity**: how connected the objects are to their closest neighbours

### Hierarchical Clustering
Hierarchical clustering (HC) is popular because of its intuitive representation using dendrograms (trees). More similar objects are joined near the bottom of a dendrogram whereas less similar objects are joined at a higher tree height. First a distance metric is used to compute a distance matrix. Then objects/features are clustered based on a linkage type. The linkage criterion determines the distances among a set of objects/features using the pairwise distances. For example, an average linkage would use the average pairwise distances. On the other hand, single linkage uses the minimum pairwise distance. A dendrogram with all objects/features can be made by recursively linking increasingly larger subsets of observations together. 

Single linkage works well for data sets exhibiting connectivity but not compactness. This is because single linkage looks for minimum pairwise distances, which would cluster together neighbouring points. An example of this would be tree rings. The clusters are circles, and objects that are far away can be in the same cluster compared to objects that are actually closer. On the other hand, average linkage works well for data sets exhibiting compactness. This works where the clusters look like non-overlapping blobs.

### K-Means and PAM
First, k means (centroids) are randomly initialized in the multidimensional object/feature space that we wish to cluster. The clusters are formed by assigning each object/feature to its closest centroid. The centroids are recalculated based on the cluster memberships and the  subsequently updated. This procedure is iterated until the centroids converge.

There are two caveats to note when using k-means. First, sometimes the cluster assignments are unstable because they depend on the random initialization of the centroids. We preferably want to repeat the algorithm many times to see whether the clusters are sensitive to the choice of initial centroid. Secondly, choosing k is not arbitrary. Cross-validation using an appropriate loss function is a popular method for choosing k. Other methods use evaluation indices, some of which we will describe later in the report.

Partitioning Around Medoids (PAM) is very similar to k-means except that we randomly initialize k random data points (medoids). Medoids must be actual data points whereas centroids can be any point in the feature space.

### Nonnegative Matrix Factorization (NMF)
Given a non-negative data matrix $A$, we can factor it into two matrices $W$ and $H$, which are also non negative. $W$ and $H$ have important properties. Suppose $A$ has genes as rows and samples as columns. If we are interested in clustering samples, then $H$ has a reduced gene space of metagenes that fully explain the samples. Samples are clustered based on the metagene they are most associated with. If we are interested in clustering genes, then $W$ has a reduced sample space of metasamples that fully explain the genes. Genes are clustered based on the metasample they are most associated with.

In gene expression data, it is common to standardize the genes. Doing so would likely disrupt the nonnegativity of $A$ required for NMF. A simple remedy can solve this problem: append the matrix $-A$ to the bottom of $A$ (preserving the same number of columns), and set all negative entries to 0. The computational complexity will have been doubled as a result. NMF takes a long time to run, but studies have shown clustering assignments can be highly stable^1^.

## Consensus Clustering
Monti et al.^2^ describe consensus clustering as an algorithm but also as a method of combining realizations of other clustering algorithms. Algorithms like k-means and PAM are unstable, as the clustering assignments depend on the initialization of centroids and medoids, respectively. Consensus clustering combines results from repeated runs of a clustering algorithm. Typically, each run uses different random subsamples of the data to model sampling variability. A final clustering assignment is determined based on agreement across replicates.

The consensus matrix is a significant aspect of consensus clustering. Consider a consensus matrix $C$. Entry $C_{ij}$ is the proportion of runs that object $i$ and object $j$ were clustered together, out of all runs in which $i$ and $j$ were both selected in subsamples, where $1\leq i,j\leq N$, $N$ = number of objects. $C$ is symmetric because $C_{ij} = C_{ji}$ (i.e. no order exists). All entries range from 0 to 1 since $C_{ij}$ are proportions. A perfect clustering would consist of a consensus matrix with only 0 (objects never clustered together) or 1 (objects always clustered together). The final step to obtaining the consensus clustering assignment is to use hierarchical clustering on the consensus matrix.

Based on the hierarchical clustering, we can plot a heatmap of the consensus matrix. Repeating this for different values of k (number of clusters) and looking at the corresponding heatmaps, we can determine which value of k provides the most stable clustering assignment. The goal is to have a well-defined diagonal block structure in the heatmap, one block per cluster. Using a performance measure such as PAC (which we will describe) would be a more formal method of assessment that doesn't rely on potentially subjective visualizations.

Consensus clustering attempts to adjust for the randomness introduced by subsampling and the clustering algorithm itself. An extension is sometimes called meta-consensus clustering, where we aggregate results across different algorithms in addition to aggregating across subsamples within an algorithm. For instance, we may combine the consensus clustering assignments from 10 different algorithms to come up with a final clustering. The choice of which algorithms to use is not trivial. We hope to prove that using meta-consensus clustering is superior to using consensus clustering of one algorithm.

# Performance Evaluation: TCGA Dataset

```{r load}
results.CCP <- readRDS("TCGA/outputs/results_CCP.rds")
results.NMF <- readRDS("TCGA/outputs/results_NMF.rds")
data(hgsc)
dat <- hgsc %>%
  set_rownames(.$UNIQID) %>%
  select(which(sapply(., class) == "numeric")) %>%
  magrittr::extract(apply(., 1, sd) > 1, ) %>%
  t %>%
  scale
```

A published dataset from TCGA^3^ uses `r nrow(hgsc)` genes to cluster `r ncol(hgsc) - 1` HGSC samples into the four subtypes: C1, C2, C4, and C5. TCGA used consensus clustering with NMF. In this report, we consider the following clustering algorithms to use in consensus clustering:

* Hierarchical Clustering with Euclidean distance
    - Average Linkage
    - Single Linkage
    - DIvisive ANAlysis (DIANA)
* K-Means
    - Euclidean distance
    - Spearman distance
    - Mutual Information distance
* PAM
    - Euclidean distance
    - Spearman distance
    - Mutual Information distance
* NMF
    - KL divergence
    - Euclidean distance
  
The number of repetitions used for consensus clustering is 1000. Each replicate uses a subsample of the data with 80% of the total number of features.

The figure below shows the meta consensus matrix across the `r length(ALG.NAMES)` algorithms, each of which used consensus clustering.
 
```{r all_clusters}
results.all <- results.CCP %>%
  sapply(., function(x) x[[4]]$consensusClass) %>%
  cbind(., results.NMF$nmfDiv$consensusClass,
        results.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# TCGA classes
true.class <- as.factor(substring(rownames(results.all), first = 18))

# Meta consensus matrix heatmap
meta.cm <- consensusMatrix(results.all)
palOrRd <- colorRampPalette(brewer.pal(n = 9, "OrRd"))(256)
heatmap.2(meta.cm, labRow = NA, labCol = NA, col = palOrRd,
          trace = "none", density.info = "none")
```

From the heatmap, we do not see a very high concordance across algorithms. For example, the proportion of cases with at least 0.6 agreement is only `r ((sum(meta.cm >= 0.6) - ncol(meta.cm)) / 2) / choose(ncol(meta.cm), 2)`. There is some evidence of a four-class data structure.

The confusion matrix is shown below for the meta consensus classes compared to TCGA's classification. Several metrics are shown for each class.

```{r conf_mat, results='asis'}
final.compare <- hclust(dist(meta.cm), method = "average") %>%
  cutree(4) %>%
  as.factor %>%
  set_names(hclust(dist(meta.cm), method = "average")$labels) %>%
  set_names(true.class) %>%
  table(., names(.)) %>%
  magrittr::extract(names(sort(apply(., 1, which.max))), ) %>%
  set_rownames(colnames(.)) %>%
  as.table

names(dimnames(final.compare)) <- c("Predicted", "Reference")
cm <- caret::confusionMatrix(final.compare)

pandoc.table(cm$table)
pandoc.table(cm$byClass)
```

## External Evaluation
External evaluation refers to the case when we compare our clustering assignments to true class labels, or have some gold standard to compare to. In applications, this might be the published clustering result. The downside of using external evaluation is that the reference classes may not be correctly clustered themselves, and we are treating these as the norm. None the less, we can explore a few metrics.

We expect that our own NMF-based algorithms to perform well on these evaluation indices because the reference classes were clustered using NMF too.

### Purity and Entropy

Purity is defined as the sum of the entities of maximal class in each cluster divided by the total number of entities^4^. The equation is:
\begin{center}
$Purity = \dfrac{1}{n}\sum\limits_{r = 1}^{k}\max\limits_{i}(n_r^i)$
\end{center}
where $n$ is the total number of entities, $k$ is the number of clusters, $i$ is a particular class, and $n_r^i$ is the number of objects classified into class $i$ in cluster $r$. The larger the purity, the better the clustering accuracy.
```{r purity, results='asis'}
Purity <- apply(results.all, 2, function(x) purity(as.factor(x), true.class)) %>%
  sort(decreasing = T) %>% 
  data.frame(Purity = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, Purity)
pandoc.table(Purity, justify = "lc")
```

Entropy measures the amount of uncertainty in each cluster^4^. The equation is:
\begin{center}
$Entropy = -\dfrac{1}{n\log{q}}\sum\limits_{r = 1}^{k}\sum\limits_{i = 1}^{q}n_r^i\log{\dfrac{n_r^i}{n_r}}$
\end{center}
The smaller the entropy, the less uncertain we are of the cluster membership, and the better the clustering performance.
```{r entropy, results='asis'}
Entropy <- apply(results.all, 2, function(x) entropy(as.factor(x), true.class)) %>% 
  sort %>% 
  data.frame(Entropy = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, Entropy)
pandoc.table(Entropy, justify = "lc")
```

### Kappa Statistics

Cohen's kappa statistic $\kappa$ measures the level of agreement between two raters^5^. In our case, the raters are different clustering algorithms.
\begin{center}
$\kappa = \dfrac{p_o - p_e}{1 - p_e}$
\end{center}

The weighted $\kappa$ statistic is as follows and takes into account the off-diagonal elements of the confusion matrix^6^:
\begin{center}
$\kappa_w = 1 - \dfrac{\sum_{i=1}^k\sum_{j=1}^k w_{ij}x_{ij}}{\sum_{i=1}^k\sum_{j=1}^k w_{ij}m_{ij}}$
\end{center}

Fleiss gave the following interpretations for $\kappa$^7^. Albeit arbitrary and not numerically determined they still serve as a rough guideline.
```{r kappa_ratings, results='asis'}
kappa_rating <- data.frame(Rating = c("Poor", "Fair", "Excellent"),
                           Kappa = c("Less than 0.40", "Between 0.40 and 0.75", "Greater than 0.75"))
pandoc.table(kappa_rating)
```

The results for the TCGA dataset are shown below:

```{r kappa_results, results='asis'}
kappas <- alply(results.all, 2, function(x)
  table(x, true.class) %>% 
    extract(names(sort(apply(., 1, which.max))), ) %>%
    set_rownames(colnames(.)), .dims = T) %>% 
  sapply(., psych::wkappa) %>% 
  as.data.frame %>% 
  mutate(Meta = psych::wkappa(final.compare)) %>% 
  t %>% 
  apply(., 2, unlist) %>% 
  as.data.frame %>%
  set_colnames(c("kappa", "weighted.kappa")) %>% 
  mutate(Algorithm = rownames(.),
         Rating = ifelse(weighted.kappa < 0.4, "Poor",
                         ifelse(weighted.kappa > 0.75, "Excellent",
                                "Fair"))) %>%
  arrange(desc(weighted.kappa)) %>% 
  select(Algorithm, kappa, weighted.kappa, Rating)

pandoc.table(kappas)
```

### Adjusted Rand Index

The Rand Index measures agreement between two classes by counting the number of pairs of objects that are clustered together in two clusterings^8^. Hubert & Araibe propose an adjusted version to account for random chance^9^. The equation for the adjusted Rand index (ARI) is shown below^8, 9, 10^:

\begin{center}
$ARI = \dfrac{\sum_{ij}\binom{n_{ij}}{2} - [\sum_i\binom{a_i}{2}\sum_j\binom{b_j}{2}] / \binom{n}{2}}{\frac{1}{2}[\sum_i\binom{a_i}{2} + \sum_j\binom{b_j}{2}] - [\sum_i\binom{a_i}{2}\sum_j\binom{b_j}{2}] / \binom{n}{2}}$
\end{center}

where $n_{ij}$'s are entries in the prediction vs. reference confusion matrix, $a_i$ and $b_j$ are row and column marginal totals respectively, and $N$ is the grand total. The ARI is 1 when the two clusterings are perfect, and 0 when there is no concordance.

```{r ARI, results='asis'}
ARI <- apply(results.all, 2, adjustedRandIndex, x = true.class) %>% 
  sort(decreasing = T) %>% 
  data.frame(ARI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, ARI)
pandoc.table(ARI, justify = "lc")
```

## Internal Evaluation

### Proportion of Ambiguously Clustered Pairs (PAC)

The notion of PAC is closely related to consensus clustering. Senbabaoglu et al. argue that PAC is a better measure of determining the optimal number of clusters from consensus matrices^11^, compared to other measures such as the Gap statistic, CLEST, etc. The idea is simple: the proportion of ambiguously clustered pairs is the number of entries in the consensus matrix that are not 0 or 1. Recall that the consensus matrix is symmetric, so we only consider the lower (or upper) triangular matrix. In most applications, the definition of ambiguity is less stringent, and any entry greater than $p$ or less than $1 - p$ contributes to PAC, where $p$ is a small proportion (e.g. 0.05).

Although Senbabaoglu et al. initially used PAC to determine the optimal number of clusters, here we are using it to compare different clustering algorithms. None the less, we want the PAC to be as small as possible. A perfect score for PAC would be 0, meaning that all entries in the corresponding consensus matrix are either 0 or 1. The advantage of using this index is that it utilizes results from consensus clustering and thus not biased towards a particular distance metric. However, a major limitation is that PAC only uses stability evidence and doesn't assess accuracy. A clustering algorithm can be consistently wrong and do well for PAC, masking the apparent performance of said algorithm to make correct classifications.

```{r PAC, results='asis'}
PAC <- results.CCP %>%
  sapply(., function(x) PAC(x[[4]]$consensusMatrix)) %>%
  c(nmfDiv = PAC(results.NMF$nmfDiv$consensusMatrix),
    nmfEucl = PAC(results.NMF$nmfEucl$consensusMatrix)) %>%
  set_names(ALG.NAMES) %>% 
  sort %>% 
  data.frame(PAC = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, PAC)
pandoc.table(PAC, justify = "lc")
```

The PAC for the meta consensus matrix is `r PAC(meta.cm)`. Not surprisingly, it appears to be a weighted average of the individual algorithm PACs.

### Davies-Bouldin Index

The Davies-Bouldin Index (DBI) measures the ratio of the within cluster scatter to the between cluster separation^12^. Hence, we want to *minimize* DBI so that objects in the same cluster are not too scattered and objects in different clusters are well separated. The advantage of using DBI to compare algorithms is that it utilizes properties of the data structure, and not true class labels. However, like the PAC, it may not indicate we are making the correct partitions.

```{r DBI, results='asis'}
DBI <- dat %>%
  apply(results.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average") %>%
  sort %>% 
  data.frame(DBI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, DBI)
pandoc.table(DBI, justify = "lc")
```

### Dunn Index

The Dunn Index (DI) is a ratio of the minimum intercluster distance to the maximum intracluster distance^13^. In this report, we use the following definitions for the cluster-specific distances:

- Intercluster distance: distance between two farthest points in different clusters
- Intracluster distance: distance between two closest points in the same cluster

As a result, the higher the DI the better the clustering assignment. Similar to the DBI, the DI uses the data itself to determine clustering performance.

```{r DI, results='asis'}
DI <- apply(results.all, 2, dunn, distance = dist(dat)) %>% 
  sort(decreasing = T) %>% 
  data.frame(DI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, DI)
pandoc.table(DI, justify = "lc")
```

### Rousseeuw's Silhouette

Rousseuw's Silhouette internal cluster quality index (RS) measures how well each object is clustered by comparing its dissimilarity with other points in its own cluster to points in its neighbouring cluster^14^. The silhouette index ranges from -1 to 1. If the index is close to 1 then the object is clustered well and if it is close to -1 then the object would be better clustered in the neighbouring cluster.

The average silhouette index measures how well *all* objects are clustered, and is the measure we use here to compare the different algorithms. Just like the individual silhouette indices, we want to maximize the RS as well.

```{r RS, results='asis'}
RS <- apply(results.all, 2, index.S, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(RS = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, RS)
pandoc.table(RS, justify = "lc")
```

### C-Index

The C-Index (CI) as proposed by Hubert & Levin is a ratio of within-cluster dissimilarities^15^. For the CI, minimizing it indicates the optimal number of clusters. Here we use it to compare algorithms.

```{r CI, results='asis'}
CI <- apply(results.all, 2, index.G3, d = dist(dat)) %>%
  sort %>% 
  data.frame(CI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, CI)
pandoc.table(CI, justify = "lc")
```

### Gamma Index

The Gamma Index described by Baker & Hubert is a ratio of the difference in the number of discordant comparisons to concordant comparisons, over all comparisons^16^. A high value for GI would thus indicate the clustering assignments have more concordance than discordance. Originally used to determine the optimal number of clusters, here we use it to compare algorithms.

```{r GI, results='asis'}
GI <- apply(results.all, 2, index.G2, d = dist(dat)) %>%
  sort(decreasing = T) %>% 
  data.frame(GI = .) %>% 
  mutate(Algorithms = rownames(.)) %>% 
  select(Algorithms, GI)
pandoc.table(GI, justify = "lc")
```

### CH Index

The Calinski-Harabasz pseudo F-statistic index (CHI) measures the ratio of the between-group dispersion matrix to the within-group dispersion matrix, each normalized by their respective degrees of freedom^17^. The CHI is called the pseudo F-statistic because the dfs are similar to the ANOVA F-statistic. A maximal CHI indicates the optimal number of clusters, yet here we use it to compare algorithms.

An advantage of the CHI over other indices is that it does not depend on a distance metric (e.g. Euclidean) in its formulation. Algorithms that use Euclidean distances would be more favoured if that were not the case.

```{r CHI, results='asis'}
CHI <- apply(results.all, 2, index.G1, x = dat) %>%
  sort(decreasing = T) %>% 
  data.frame(CHI = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, CHI)
pandoc.table(CHI, justify = "lc")
```

### Connectivity

Connectivity measures how connected the clusters are based on its nearest neighbours^18^. The measure ranges from 0 to infinity and a small value indicates high connectivity.

```{r Connectivity, results='asis'}
Conn <- apply(results.all, 2, clValid::connectivity, distance = dist(dat)) %>% 
  sort %>% 
  data.frame(Conn = .) %>% 
  dplyr::mutate(Algorithms = rownames(.)) %>% 
  dplyr::select(Algorithms, Conn)
pandoc.table(Conn, justify = "lc")
```

### Summary

Here is a summary of all the indices for each algorithm, in unsorted order.

```{r index_summary, results='asis'}
index.summary <- Reduce(merge, list(PAC, DBI, DI, RS, CI, GI, CHI, Conn))
pandoc.table(index.summary,
             justify = paste0("l",
                              paste(rep("c", ncol(index.summary) - 1),
                                    collapse = "")))
```

## Ranked Indices

The table below shows the ranking of algorithms for performance on a clustering index, for each index. There is an additional column that shows the proportion of indices where an algorithm was ranked **first or second**.

```{r ranked_indices, results='asis'}
ranked.indices <- data.frame(Algorithms = index.summary$Algorithms,
                             PAC = rank(index.summary$PAC),
                             DBI = rank(index.summary$DBI),
                             DI = rank(-index.summary$DI),
                             RS = rank(-index.summary$RS),
                             CI = rank(index.summary$CI),
                             GI = rank(-index.summary$GI),
                             CHI = rank(-index.summary$CHI),
                             Conn = rank(index.summary$Conn)) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
  
pandoc.table(ranked.indices,
             justify = paste0("l",
                              paste(rep("c", ncol(ranked.indices) - 1),
                                    collapse = "")))
```

If we were to conduct a meta consensus clustering, a weight for each algorithm needs to be assigned. One such way of doing so is using the inverse rank sums. We can sum the ranks for each algorithm, and assign higher weight for consistently high ranked methods (1st or 2nd) and vice versa. This is shown below:

```{r inverse_rank_sum, results='asis'}
ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 10)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)

pandoc.table(ranked.sums,
             justify = paste0("l",
                              paste(rep("c", ncol(ranked.sums) - 1),
                                    collapse = "")))
```

# Simulations

To confirm the clustering results from using the TCGA dataset, we need to try out the algorithms on a few simulated datasets, designed to test the robustness of each method. The `clusterSim` package provides very good built-in examples to use.

## Worms Dataset

The following two-cluster dataset is our first simulation:

```{r worms_plot}
set.seed(1)
sw <- shapes.worms()
dat <- sw$data
true.class <- sw$clusters
plot(dat, col = c(2:3)[true.class], xlab = "x", ylab = "y",
     main = "Two clusters with atypical parabolic shapes (worms)")
```

```{r worms_index}
# combine results
worms.CCP <- readRDS("Worms/outputs/results_CCP.rds")
worms.NMF <- readRDS("Worms/outputs/results_NMF.rds")
worms.all <- worms.CCP %>%
  sapply(., function(x) x[[2]]$consensusClass) %>%
  cbind(., worms.NMF$nmfDiv$consensusClass,
        worms.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES)

# indices
PAC <- worms.CCP %>%
  sapply(., function(x) PAC(x[[2]]$consensusMatrix, 0.05, 0.95)) %>%
  c(nmfDiv = PAC(worms.NMF$nmfDiv$consensusMatrix, 0.05, 0.95),
    nmfEucl = PAC(worms.NMF$nmfEucl$consensusMatrix, 0.05, 0.95)) %>% 
  set_names(ALG.NAMES)
DBI <- dat %>%
  apply(worms.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average")
DI <- apply(worms.all, 2, dunn, distance = dist(dat))
RS <- apply(worms.all, 2, index.S, d = dist(dat))
CI <- apply(worms.all, 2, index.G3, d = dist(dat))
GI <- apply(worms.all, 2, index.G2, d = dist(dat))
CHI <- apply(worms.all, 2, index.G1, x = dat)
Conn <- apply(worms.all, 2, clValid::connectivity, distance = dist(dat))
```

The ranked indices and weights tables are shown below:

```{r worms_summary, results='asis'}
ranked.indices <- data.frame(Algorithms = ALG.NAMES,
                             PAC = rank(PAC, ties.method = "min"),
                             DBI = rank(DBI, ties.method = "min"),
                             DI = rank(-DI, ties.method = "min"),
                             RS = rank(-RS, ties.method = "min"),
                             CI = rank(CI, ties.method = "min"),
                             GI = rank(-GI, ties.method = "min"),
                             CHI = rank(-CHI, ties.method = "min"),
                             Conn = rank(Conn, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
pandoc.table(ranked.indices,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.indices) - 1),
                                            collapse = "")))

ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 10)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)
pandoc.table(ranked.sums,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.sums) - 1),
                                            collapse = "")))
```

Using hierarchical clustering with Wald's method on the meta-consensus matrix across the ten algorithms, we can obtain meta-consensus classes. The following table shows how the different methods compare, based on the number of matches to the true class labels.

```{r worms_meta, results='asis'}
weights <- with(ranked.sums, Sum^-1 / sum(Sum^-1))[match(colnames(worms.all), ranked.sums$Algorithms)]
clust.1 <- apply(worms.all, 1, function(x) which(x == 1))
clust.2 <- apply(worms.all, 1, function(x) which(x == 2))
worms.final <- worms.all %>%
  as.data.frame %>% 
  mutate(weight.1 = sapply(clust.1, function(x) sum(weights[x])),
         weight.2 = sapply(clust.2, function(x) sum(weights[x])),
         Meta = ifelse(weight.1 > weight.2, 1, 2)) %>% 
  select(-weight.1, -weight.2)

meta.classes <- as.factor(cutree(hclust(dist(consensusMatrix(worms.all)), method = "ward.D"), 2))

worms.matches <- worms.final %>% 
  apply(., 2, function(x) sum(x == true.class)) %>% 
  sort(decreasing = T) %>% 
  data.frame(Matches = .)

pandoc.table(worms.matches)
```

Let's visualize how the best algorithm, hierarchical clustering using single linkage and Euclidean distance, separates the clusters:

```{r worms_best}
plot(sw$data, col = c(2:3)[worms.final$`HC (Sing Euc)`], xlab = "x", ylab = "y",
     main = "K-Means (euclidean) clustering of two atypical parabolic shapes (worms)")
```

It appears that the best algorithm can separate the clusters, yet the indices do not favour such a partition and it appears that the algorithm is performing poorly.

## Rings Dataset

The following three-cluster dataset is our second simulation:

```{r rings_plot}
set.seed(1)
sc3 <- shapes.circles3()
dat <- sc3$data
true.class <- sc3$clusters
plot(dat, col = c(2:4)[true.class], xlab = "x", ylab = "y",
     main = "Three clusters with atypical ring shapes (circles)")
```

```{r rings_index}
# combine results
rings.CCP <- readRDS("Rings/outputs/results_CCP.rds")
rings.NMF <- readRDS("Rings/outputs/results_NMF.rds")
rings.all <- rings.CCP %>%
  sapply(., function(x) x[[3]]$consensusClass) %>%
  cbind(., rings.NMF$nmfDiv$consensusClass,
        rings.NMF$nmfEucl$consensusClass) %>% 
  set_colnames(ALG.NAMES[-8])

# indices
PAC <- rings.CCP %>%
  sapply(., function(x) PAC(x[[3]]$consensusMatrix)) %>%
  c(nmfDiv = PAC(rings.NMF$nmfDiv$consensusMatrix),
    nmfEucl = PAC(rings.NMF$nmfEucl$consensusMatrix)) %>% 
  set_names(ALG.NAMES[-8])
DBI <- dat %>%
  apply(rings.all, 2, cls.scatt.data, data = .) %>%
  sapply(., clv.Davies.Bouldin, intracls = "average", intercls = "average")
DI <- apply(rings.all, 2, dunn, distance = dist(dat))
RS <- apply(rings.all, 2, index.S, d = dist(dat))
CI <- apply(rings.all, 2, index.G3, d = dist(dat))
GI <- apply(rings.all, 2, index.G2, d = dist(dat))
CHI <- apply(rings.all, 2, index.G1, x = dat)
Conn <- apply(rings.all, 2, clValid::connectivity, distance = dist(dat))
```

```{r rings_summary, results='asis'}
ranked.indices <- data.frame(Algorithms = ALG.NAMES[-8],
                             PAC = rank(PAC, ties.method = "min"),
                             DBI = rank(DBI, ties.method = "min"),
                             DI = rank(-DI, ties.method = "min"),
                             RS = rank(-RS, ties.method = "min"),
                             CI = rank(CI, ties.method = "min"),
                             GI = rank(-GI, ties.method = "min"),
                             CHI = rank(-CHI, ties.method = "min"),
                             Conn = rank(Conn, ties.method = "min")) %>% 
  mutate(Top = apply(.[, -1], 1,
                     function(x) paste0(round(100 * sum(x %in% c(1, 2)) /
                                                ncol(.[, -1]), 2), "%"))) %>%
  arrange(desc(Top))
pandoc.table(ranked.indices,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.indices) - 1),
                                            collapse = "")))

ranked.sums <- ranked.indices %>% 
  mutate(Sum = apply(.[, -c(1, 10)], 1, sum),
         Weight = paste0(round(100 * Sum^-1 / sum(Sum^-1), 2), "%")) %>% 
  select(Algorithms, Top, Sum, Weight) %>% 
  arrange(Sum)
pandoc.table(ranked.sums,
                     justify = paste0("l",
                                      paste(rep("c", ncol(ranked.sums) - 1),
                                            collapse = "")))
```

```{r rings_meta, results='asis'}
# weights <- with(ranked.sums, Sum^-1 / sum(Sum^-1))[match(colnames(rings.all), ranked.sums$Algorithms)]
# clust.1 <- apply(rings.all, 1, function(x) which(x == 1))
# clust.2 <- apply(rings.all, 1, function(x) which(x == 2))
# clust.3 <- apply(rings.all, 1, function(x) which(x == 3))
# rings.final <- rings.all %>%
#   as.data.frame %>% 
#   mutate(weight.1 = sapply(clust.1, function(x) sum(weights[x])),
#          weight.2 = sapply(clust.2, function(x) sum(weights[x])),
#          weight.3 = sapply(clust.3, function(x) sum(weights[x])),
#          Meta = ifelse(weight.1 > weight.2 & weight.1 > weight.3, 1,
#                        ifelse(weight.2 > weight.1 & weight.2 > weight.3, 2, 3))) %>% 
#   select(-weight.1, -weight.2, -weight.3)

# meta.classes <- as.factor(cutree(hclust(dist(consensusMatrix(rings.all)), method = "ward.D"), 2))
rings.final <- rings.all %>% 
  as.data.frame %>% 
  mutate(Meta = as.factor(cutree(hclust(dist(consensusMatrix(rings.all)), method = "ward.D"), 3)))

rings.matches <- rings.final %>% 
  apply(., 2, function(x) sum(x == true.class)) %>% 
  sort(decreasing = T) %>% 
  data.frame(Matches = .)

pandoc.table(rings.matches)
```

```{r rings_best}
plot(dat, col = c(2:4)[rings.final$`HC (Sing Euc)`], xlab = "x", ylab = "y",
     main = "HC (single linkage euclidean) clustering")
```

Again, the hierarchical clustering using single linkage and euclidean distance perfectly separates the feature space into the three rings.

## Seeds


# References

1. Brunet, J. P., Tamayo, P., Golub, T. R., & Mesirov, J. P. (2004). "Metagenes and molecular pattern discovery using matrix factorization." *Proceedings of the national academy of sciences*, 101 (12), 4164-4169.

2. Monti, S., Tamayo, P., Mesirov, J., & Golub, T. (2003). "Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data." *Machine learning*, 52 (1-2), 91-118.

3. Cancer Genome Atlas Research Network. (2011). "Integrated genomic analyses of ovarian carcinoma." *Nature*, 474 (7353), 609-615.

4. Conrad, J. G., Al-Kofahi, K., Zhao, Y., & Karypis, G. (2005, June). "Effective document clustering for large heterogeneous law firm collections." In *Proceedings of the 10th international conference on Artificial intelligence and law* (pp. 177-187). ACM.

5. Cohen, J. (1960). "A coefficient of agreement for nominal scales." *Educational and psychological measurement*, 20 (1), 37-46.

6. Cohen, J. (1968). "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit." *Psychological bulletin*, 70 (4), 213.

7. Fleiss, J.L. (1981). *Statistical methods for rates and proportions* (2nd ed.). New York: John Wiley.

8. Rand, W. M. (1971). "Objective criteria for the evaluation of clustering methods." *Journal of the American Statistical Association*, 66 (336), 846-850.

9. Hubert, L., & Arabie, P. (1985). "Comparing partitions." *Journal of Classification*, 2 (1), 193-218.

10. Vinh, N. X., Epps, J., & Bailey, J. (2009, June). "Information theoretic measures for clusterings comparison: is a correction for chance necessary?." *In Proceedings of the 26th Annual International Conference on Machine Learning* (pp. 1073-1080). ACM.

11. Senbabaoglu, Y., Michailidis, G., & Li, J. Z. (2014). "Critical limitations of consensus clustering in class discovery." *Scientific reports*, 4.

12. Davies, David L.; Bouldin, Donald W. (1979). "A Cluster Separation Measure." *IEEE Transactions on Pattern Analysis and Machine Intelligence*. PAMI-1 (2): 224–227.

13. Dunn, J. C. (1973). "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters." *Journal of Cybernetics* 3 (3): 32–57.

14. Rousseeuw, P. J. (1987). "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis." *Journal of Computational and Applied Mathematics*, 20, 53-65.

15. Hubert, L. J., & Levin, J. R. (1976). "A general statistical framework for assessing categorical clustering in free recall." *Psychological bulletin*, 83 (6), 1072-1080.

16. Baker, F. B., & Hubert, L. J. (1975). "Measuring the power of hierarchical cluster analysis." *Journal of the American Statistical Association*, 70 (349), 31-38.

17. Calinski, T., & Harabasz, J. (1974). "A dendrite method for cluster analysis." *Communications in Statistics-theory and Methods*, 3 (1), 1-27.

18. Handl, J., Knowles, J., & Kell, D. B. (2005). "Computational cluster validation in post-genomic data analysis." *Bioinformatics*, 21 (15), 3201-3212.

